{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "My KNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/patternproject/p.PakistanAI-BootCamp/blob/Week-3/MyAttempt/My_KNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dr8ZF1AeiPob",
        "colab_type": "text"
      },
      "source": [
        "# Revision History\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   v1: First Version \n",
        "*   v2: Adding logging to trace what is happening in OOP \n",
        "*   v3: changing the logging time stamp to local zone i.e Karachi\n",
        "*   v4: debug statements added across all the functions of class KNearestNeighbor\n",
        "*   v5: No Change \n",
        "*   v6: Minor changes\n",
        "*   v7: One Loop Variant - implemented\n",
        "*   v8: Zero Loop Variant - impleneted. Though we get assertion error that calculation of Zero Loop and Two Loops are too different, hence need to be fixed. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tryVyVWEJUg9",
        "colab_type": "text"
      },
      "source": [
        "# References\n",
        "\n",
        "1.   Thoughtful Machine Learning \n",
        "\n",
        "Has a chapter on KNN. Available online and on system\n",
        "https://doc.lagout.org/science/Artificial%20Intelligence/Machine%20learning/Thoughtful%20Machine%20Learning_%20A%20Test-Driven%20Approach%20%5BKirk%202014-10-12%5D.pdf\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1CJpJsKX5Vf",
        "colab_type": "text"
      },
      "source": [
        "# K Nearest Neighbors\n",
        "### *Written By: Sibt ul Hussain*\n",
        "----\n",
        "## Goal\n",
        "\n",
        "Your goal in this assigment is to implement a KNN Classifier.\n",
        "\n",
        "**Note** Please note that you are allowed to use only those libraries which we have discussed in the workshop, i.e. numpy and pandas.\n",
        "\n",
        "**Please read each step carefully and understand it fully before proceeding with code writing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYa9pYGIX5Vj",
        "colab_type": "text"
      },
      "source": [
        "Lets suppose there are $m$ examples of size $d$, which are stored in a training matrix $D$ of size $m \\times d$. Now if we use Euclidean distance for the finding the best match for a given test example  $q_{d\\times1}$ how many multiplications and additions we will need to perform ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cekDkaqWX5Vl",
        "colab_type": "text"
      },
      "source": [
        "Here you will be implementing three versions of KNN classifiers, those are:\n",
        " 1. Using two nested loops, i.e. for a given example you will have to compute its L2 distance with all the training samples, thus you will need to implement it using two nested loops...**Please first complete the assignment using this method and then go for further optimizations mentioned in step 2 and 3**\n",
        " 2. Using one loop, i.e. you can get rid of one loop using the numpy vectorization (tiling the test example).\n",
        " 3. The number of multiplications and additions can be reduced a lot if we use an alternate definition of Euclidean distance, i.e. $$||x-y||^2=||x||^2+||y||^2-2x^Ty$$\n",
        "instead of using the traditional definition $$||x-y||^2=\\sum_i (x_i-y_i)^2$$ This definition will help you to implement a very fast version of KNN classifier without using any loop whatsoever. \n",
        "\n",
        "#### HelpFul Functions\n",
        "You might find following functions to be extremely helpful\n",
        " - **[argpartition] (http://docs.scipy.org/doc/numpy/reference/generated/numpy.argpartition.html)** uses introselect algorithm to perform an indirect partition along the given axis. This can lead to O(n) complexity instead of O(nlogn). Remember indexing start from 0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqovkoCaX5Vm",
        "colab_type": "text"
      },
      "source": [
        "### K Nearest Neighbour Classifier\n",
        "\n",
        "Now in this assignment we will be implementing the K Nearest Neighbour Classifier for Continuous attributes.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "FexecwKfX5Vo",
        "colab_type": "code",
        "outputId": "7091773b-f610-407a-c40f-64dc04589b00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%pylab inline\n",
        "plt.style.use('ggplot')\n",
        "matplotlib.rcParams['figure.figsize'] = (10.0, 8.0)\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ml8_zmOJCQTK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for getting the right time zone\n",
        "from datetime import datetime\n",
        "import time\n",
        "\n",
        "from pytz import timezone, utc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pK9B-xgHDgCr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import pytz\n",
        "#print(pytz.all_timezones) # 'Asia/Karachi' --> what we want"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CB8IX1KiCVF6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import logging"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4d-DcFvQEjFF",
        "colab_type": "text"
      },
      "source": [
        "changing the logging time stamp to local zone i.e Karachi\n",
        "\n",
        "SRC: https://stackoverflow.com/questions/32402502/how-to-change-the-time-zone-in-python-logging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MHdtFTgCW_7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#logging.basicConfig(format=\"%(asctime)s %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "# The filemode is set to w, which means the log file is opened in “write mode” each time basicConfig() is called, and each run of the program will rewrite the file. \n",
        "logging.basicConfig(filename='example.log',filemode='w', level=logging.DEBUG,format='%(asctime)s %(message)s', datefmt=\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.error(\"default\")\n",
        "\n",
        "logging.Formatter.converter = time.localtime\n",
        "logger.error(\"localtime\")\n",
        "\n",
        "logging.Formatter.converter = time.gmtime\n",
        "logger.error(\"gmtime\")\n",
        "\n",
        "def customTime(*args):\n",
        "  utc_dt = utc.localize(datetime.utcnow())\n",
        "  #my_tz = timezone(\"US/Eastern\")\n",
        "  my_tz = timezone(\"Asia/Karachi\")\n",
        "  converted = utc_dt.astimezone(my_tz)\n",
        "  return converted.timetuple()\n",
        "\n",
        "logging.Formatter.converter = customTime\n",
        "logger.error(\"customTime\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqDa3ntCnfy-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The filemode is set to w, which means the log file is opened in “write mode” each time basicConfig() is called, and each run of the program will rewrite the file. \n",
        "#logging.basicConfig(filename='example.log',filemode='w', level=logging.DEBUG,format='%(asctime)s %(message)s')\n",
        "\n",
        "\n",
        "# this 'w' flag does not seem to work for colab. you have to restart the run-time to get a new file \"example.log\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWFTaSKcpn23",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logging.debug('Logging Started')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuMugfvc8fck",
        "colab_type": "code",
        "outputId": "5c75b98e-f2da-4a69-a377-83eee2a47806",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "! cat example.log"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-03-06 22:20:09 default\n",
            "2020-03-06 22:20:09 localtime\n",
            "2020-03-06 22:20:09 gmtime\n",
            "2020-03-07 03:20:09 customTime\n",
            "2020-03-07 03:20:09 Logging Started\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FEuoy2hygoA",
        "colab_type": "text"
      },
      "source": [
        "(above you should only see, till \"Logging Started\" \n",
        "\n",
        "if you see more, it means log file is appending to an older version. Easiest in colab is to restart the runtime, which will enforce the log file to be created from scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6fTWR9oFCpM",
        "colab_type": "text"
      },
      "source": [
        "Implmentation of KNearestNeighbor Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "checksum": "59f09954abcbb6d74cb44946b9986f85",
          "grade": false,
          "grade_id": "class",
          "locked": false,
          "solution": true
        },
        "scrolled": false,
        "id": "IkWw1tQjX5Vv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: You have to implement the following class\n",
        "\n",
        "class KNearestNeighbor:\n",
        "    ''' Implements the KNearest Neigbours For Classification... '''\n",
        "    def __init__(self, k, scalefeatures=False):        \n",
        "        logging.debug(f'>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>')\n",
        "        logging.debug(f'Entering: __init__ of kNearestNeighbor')\n",
        "        logging.debug(f'with k=, {k}')\n",
        "        self.k=k\n",
        "        self.scalefeatures=scalefeatures\n",
        "        logging.debug(f'Leaving: __init__ of kNearestNeighbor')\n",
        "        logging.debug(f'-------------------------------------')\n",
        "        \n",
        "    \n",
        "    def compute_distances_two_loops(self, X):\n",
        "        \"\"\"\n",
        "        Compute the distance between each test point in X and each training point\n",
        "        in self.X_train using a nested loop over both the training data and the \n",
        "        test data.\n",
        "\n",
        "        Input:\n",
        "        X - An num_test x dimension array where each row is a test point.\n",
        "\n",
        "        Output:\n",
        "        dists - A num_test x num_train array where dists[i, j] is the distance\n",
        "                between the ith test point and the jth training point.\n",
        "        \"\"\"\n",
        "        logging.debug(f'>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>')\n",
        "        logging.debug('Entering: compute_distances_two_loops()')\n",
        "        #logging.debug(f'X passed is) {X}')\n",
        "        \n",
        "        num_test = X.shape[0]\n",
        "        num_train = self.X_train.shape[0]\n",
        "        dists = np.zeros((num_test, num_train))\n",
        "        for i in range(num_test):\n",
        "            for j in range(num_train):\n",
        "                #####################################################################\n",
        "                # TODO:                                                             #\n",
        "                # Compute the l2 distance between the ith test point and the jth    #\n",
        "                # training point, and store the result in dists[i, j]               #\n",
        "                #####################################################################\n",
        "                # YOUR CODE HERE\n",
        "                #print(i)\n",
        "                #print(j)\n",
        "                logging.debug(f'ith test point: {X[i]}')\n",
        "                logging.debug(f'jth training point: {self.X_train[j]}')\n",
        "                logging.debug(f'l2 distance: {np.linalg.norm(X[i]-self.X_train[j])}')\n",
        "                dists[i][j] = np.linalg.norm(X[i]-self.X_train[j])\n",
        "                #pass\n",
        "\n",
        "                #####################################################################\n",
        "                #                       END OF YOUR CODE                            #\n",
        "                #####################################################################\n",
        "        \n",
        "        logging.debug(f'Leaving: compute_distances_two_loop()')\n",
        "        logging.debug(f'-------------------------------------')\n",
        "        return dists\n",
        "\n",
        "    def compute_distances_one_loop(self, X):\n",
        "        \"\"\"\n",
        "        Compute the distance between each test point in X and each training point\n",
        "        in self.X_train using a single loop over the test data.\n",
        "\n",
        "        Input / Output: Same as compute_distances_two_loops\n",
        "        \"\"\"\n",
        "        logging.debug(f'>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>')\n",
        "        logging.debug('Entering: compute_distances_one_loop()')\n",
        "        num_test = X.shape[0]\n",
        "        num_train = self.X_train.shape[0]\n",
        "        dists = np.zeros((num_test, num_train))\n",
        "        for i in range(num_test):\n",
        "            #######################################################################\n",
        "            # TODO:                                                               #\n",
        "            # Compute the l2 distance between the ith test point and all training #\n",
        "            # points, and store the result in dists[i, :].                        #\n",
        "            #######################################################################\n",
        "            # YOUR CODE HERE\n",
        "            \n",
        "            logging.debug(f'ith test point: {X[i]}')\n",
        "            #logging.debug(f'training matrix: {self.X_train}') # too much clutter, prints the whole matrix\n",
        "            #logging.debug(f'l2 distance via norm: {np.linalg.norm(X[i]-self.X_train)}') # this is not correct for sure\n",
        "            logging.debug(f'l2 distance via np form: {np.sqrt(np.sum(np.square(X[i] - self.X_train),axis=1))}')\n",
        "            dists[i,:] = np.sqrt(np.sum(np.square(X[i] - self.X_train),axis=1))\n",
        "            #pass\n",
        "            #######################################################################\n",
        "            #                         END OF YOUR CODE                            #\n",
        "            #######################################################################\n",
        "        \n",
        "        logging.debug(f'Leaving: compute_distances_one_loop()')\n",
        "        logging.debug(f'-------------------------------------')\n",
        "\n",
        "        return dists\n",
        "\n",
        "    def compute_distances_no_loops(self, X):\n",
        "        \"\"\"\n",
        "        Compute the distance between each test point in X and each training point\n",
        "        in self.X_train using no explicit loops.\n",
        "\n",
        "        Input / Output: Same as compute_distances_two_loops\n",
        "        \"\"\"\n",
        "        \n",
        "        logging.debug(f'>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>')\n",
        "        logging.debug('Entering: compute_distances_no_loops()')\n",
        "\n",
        "        num_test = X.shape[0]\n",
        "        num_train = self.X_train.shape[0]\n",
        "        dists = np.zeros((num_test, num_train)) \n",
        "        #########################################################################\n",
        "        # TODO:                                                                 #\n",
        "        # Compute the l2 distance between all test points and all training      #\n",
        "        # points without using any explicit loops, and store the result in      #\n",
        "        # dists.                                                                #\n",
        "        # HINT: Try to formulate the l2 distance using matrix multiplication    #\n",
        "        #       and two broadcast sums.   \n",
        "        #                                                                       #\n",
        "        #########################################################################\n",
        "        # YOUR CODE HERE\n",
        "        # no logging of values as this would be too much clutter\n",
        "        dists = np.linalg.norm(X,2) + np.linalg.norm(self.X_train,2) - 2*(np.dot(X,self.X_train.T))\n",
        "        #dists = np.linalg.norm(X) + np.linalg.norm(self.X_train) - 2*(np.dot(self.X_train,X.T))\n",
        "\n",
        "        #pass\n",
        "        #########################################################################\n",
        "        #                         END OF YOUR CODE                              #\n",
        "        #########################################################################\n",
        "        \n",
        "        \n",
        "        logging.debug(f'Leaving: compute_distances_no_loops()')\n",
        "        logging.debug(f'-------------------------------------')\n",
        "\n",
        "        return dists\n",
        "    \n",
        "    \n",
        "    def scale_features(self,X):\n",
        "        \"\"\"\n",
        "            Normalize each feature to lie in the range [0 ,1]\n",
        "\n",
        "            Input:\n",
        "            ------\n",
        "\n",
        "                X= M x d dimensional data matrix\n",
        "\n",
        "            Returns:\n",
        "            --------\n",
        "\n",
        "                normalized X\n",
        "        \"\"\"\n",
        "        \n",
        "        logging.debug(f'>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>')\n",
        "        logging.debug('Entering: scale_features()')\n",
        "\n",
        "        # we will store these values compute on training set to use during testing \n",
        "        self.xmin= np.min(X,axis=0)\n",
        "        self.xmax= np.max(X,axis=0)\n",
        "\n",
        "        logging.debug(f'Leaving: scale_features()')\n",
        "        logging.debug(f'-------------------------------------')\n",
        "\n",
        "        return (X-self.xmin)/(self.xmax-self.xmin)\n",
        "\n",
        "\n",
        "    def train(self, X, Y):\n",
        "        ''' Train K Nearest Neighbour classifier using the given \n",
        "            X [m x d] data matrix and Y labels matrix\n",
        "            \n",
        "            Input:\n",
        "            ------\n",
        "            X: [m x d] a data matrix of m d-dimensional examples.\n",
        "            Y: [m x 1] a label vector.\n",
        "            \n",
        "            Returns:\n",
        "            -----------\n",
        "            Nothing\n",
        "            '''\n",
        "        \n",
        "        logging.debug(f'>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>')\n",
        "        logging.debug(f'Entering: train() kNearestNeighbor')\n",
        "        \n",
        "        nexamples,nfeatures=X.shape\n",
        "        \n",
        "        if self.scalefeatures:\n",
        "            X=self.scale_features(X)\n",
        "        \n",
        "        # YOUR CODE HERE\n",
        "        #define self.X_train to store the training data...\n",
        "        self.X_train = X\n",
        "        \n",
        "        #define self.Y_train to store the training labels\n",
        "        self.Y_train = Y\n",
        "\n",
        "        logging.debug(f'Leaving: train() kNearestNeighbor')\n",
        "        logging.debug(f'-------------------------------------')\n",
        "    \n",
        "    def predict(self, X, methodtype='noloops'):\n",
        "        \n",
        "        \"\"\"\n",
        "        Test the trained K-Nearset Neighoubr classifier result on the given examples X\n",
        "        \n",
        "                   \n",
        "            Input:\n",
        "            ------\n",
        "            X: [m x d] a matrix of m  d-dimensional test examples.\n",
        "            methodtype: which method to use for calculating distances.\n",
        "               noloops: without using any loop\n",
        "               oneloop: using one loop\n",
        "               twoloops: using two nested loops...\n",
        "               \n",
        "            Returns:\n",
        "            -----------\n",
        "                pclass: the predicted class for the given set of examples, i.e. to which it belongs\n",
        "        \"\"\"\n",
        "        \n",
        "        logging.debug(f'>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>')\n",
        "        logging.debug('Entering: predict()')\n",
        "        \n",
        "        num_test = X.shape[0]\n",
        "        \n",
        "        if self.scalefeatures:\n",
        "            X=(X-self.xmin)/(self.xmax-self.xmin)\n",
        "        \n",
        "        y_pred = np.zeros(num_test, dtype = self.Y_train.dtype)\n",
        "        \n",
        "        # defining a function variable so that you will only need to call compute_distance...\n",
        "        if methodtype == 'noloops':\n",
        "            compute_distance = self.compute_distances_no_loops\n",
        "        elif methodtype == 'oneloop':\n",
        "            compute_distance = self.compute_distances_one_loop\n",
        "        else:\n",
        "            compute_distance = self.compute_distances_two_loops\n",
        "        \n",
        "        dists=compute_distance(X)\n",
        "        # YOUR CODE HERE\n",
        "        #y_pred = dists\n",
        "        # for each vector, we will have a row of values in dists matrix\n",
        "        # thus for each row:\n",
        "        # find the min value in row (and then get the index of this min row)\n",
        "        # now find the label of value in this index in the train data set\n",
        "        \n",
        "        # YOUR CODE END HERE\n",
        "\n",
        "        logging.debug(f'Leaving: predict()')\n",
        "        logging.debug(f'-------------------------------------')\n",
        "        \n",
        "        return y_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "B1bVbP55X5V0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import tools as t # set of tools for plotting, data splitting, etc.."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vdHPfvcpyLG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logging.debug('Importing Libraries')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Z7zY7RAJX5V_",
        "colab_type": "code",
        "outputId": "815b9e30-502e-4e9e-8383-15e7a9756508",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "source": [
        "#load the data set\n",
        "data=pd.read_csv('./iris.data')\n",
        "data.columns=['SepalLength','SepalWidth','PetalLength','PetalWidth','Class']\n",
        "print(data.describe())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       SepalLength  SepalWidth  PetalLength  PetalWidth\n",
            "count   149.000000  149.000000   149.000000  149.000000\n",
            "mean      5.848322    3.051007     3.774497    1.205369\n",
            "std       0.828594    0.433499     1.759651    0.761292\n",
            "min       4.300000    2.000000     1.000000    0.100000\n",
            "25%       5.100000    2.800000     1.600000    0.300000\n",
            "50%       5.800000    3.000000     4.400000    1.300000\n",
            "75%       6.400000    3.300000     5.100000    1.800000\n",
            "max       7.900000    4.400000     6.900000    2.500000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0xbAeZBp2d-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logging.debug('Read the data set')\n",
        "logging.debug(f'data read is \\n {data.describe()}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Qy5E9umMX5WD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get your data in matrix\n",
        "X=np.asarray(data[['SepalLength','SepalWidth','PetalLength','PetalWidth']].dropna())\n",
        "Y=np.asarray(data['Class'].dropna())\n",
        "# print(\" Data Set Dimensions=\", X.shape, \" True Class labels dimensions\", Y.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_9M9kpop7w3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logging.debug('Converted to numpy arrays')\n",
        "logging.debug('Dropped na')\n",
        "logging.debug(f'Data Set Dimensions= {X.shape}, True Class labels dimensions={Y.shape}')\n",
        "\n",
        "\n",
        "#logging.debug(\" Data Set Dimensions=\", X.shape, \" True Class labels dimensions\", Y.shape)\n",
        "#logging.debug(\" Data Set Dimensions= {} True Class labels dimensions {}\".format(X.shape, Y.shape))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "g9I5HuTMX5WP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split your data into training and test-set... \n",
        "# see the documentation of split_data in tools for further information...\n",
        "Xtrain,Ytrain,Xtest,Ytest=t.split_data(X,Y)\n",
        "\n",
        "#print(\" Training Data Set Dimensions=\", Xtrain.shape, \"Training True Class labels dimensions\", Ytrain.shape)   \n",
        "#print(\" Test Data Set Dimensions=\", Xtest.shape, \"Test True Class labels dimensions\", Ytest.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCbI006BqE_H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logging.debug('Split into train and test sets')\n",
        "#logging.debug(\" Training Data Set Dimensions=\", Xtrain.shape, \"Training True Class labels dimensions\", Ytrain.shape)   \n",
        "#logging.debug(\" Test Data Set Dimensions=\", Xtest.shape, \"Test True Class labels dimensions\", Ytest.shape)\n",
        "#logging.error(f'{name} raised an error')\n",
        "logging.debug(f'Test Data Set Dimensions=, {Xtest.shape} Test True Class labels dimensions {Ytest.shape}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmfDkcj2X5WV",
        "colab_type": "text"
      },
      "source": [
        "# Training Time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "IVEilxk-X5WX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Lets train a Decision Tree Classifier on Petal Length and Width\n",
        "feat=[0,1]\n",
        "knn=KNearestNeighbor(3) # train a 3-nearest neighbour classifier...\n",
        "knn.train(Xtrain[:,feat],Ytrain)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gglSqL9yX5Wb",
        "colab_type": "text"
      },
      "source": [
        "### Testing Your Distance Implementation and Timing (Profiling it)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puKT2tZYX5Wd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#first lets compute distance of each test example from all the training examples using two loops\n",
        "# logging.debug('This message should go to the log file')\n",
        "dists = knn.compute_distances_two_loops(Xtest[:,feat])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tflI9S7EX5Wl",
        "colab_type": "code",
        "outputId": "b9a23074-6759-4ed1-bd06-71f83aae6348",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        }
      },
      "source": [
        "print(dists.shape)\n",
        "# We can visualize the distance matrix: each row is a single test example and its distances to training examples\",\n",
        "plt.imshow(dists, interpolation='none')\n",
        "plt.gray()\n",
        "#here dark means less distance and light means high distance..."
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(45, 104)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlAAAAESCAYAAAAymuu/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nOyde7xWVbnvfy9L7iqwkEss5CqiIKIG\ngldQyUysQ9n2Xkmn7U47urVP7Tjl1n3cdkST4OTGtE+Ffuqckt3ecdLq1F4ikCACAl5AQRTwgrgU\nFgYKLtfl/MGer1PG93G9UwzRft9/eNfDHHOMOW5zzDGeS6mlpaVFxhhjjDGmYtp80AUwxhhjjPmw\n4QWUMcYYY0xBvIAyxhhjjCmIF1DGGGOMMQXxAsoYY4wxpiBeQBljjDHGFOSAvUm8cuVKzZo1S83N\nzTrjjDM0adKk96tcxhhjjDH7Le95B6q5uVk/+clP9O1vf1vTp0/XwoUL9cILL7yfZTPGGGOM2S95\nzztQ69atU+/evdWrVy9J0oknnqilS5eqb9++75rukEMOkSTV1tZqwoQJamhowOuqq6srKkfnzp1R\n/sorrySyHTt2VHRPSTrggLRq2rTh9eYRRxyRyF577bVE9uqrr2L6rA7zUFl79+6N6Xfu3JnI6Pkl\n6ZOf/GQi+/Wvf53I8s+ftZUkdezYMbl2+PDhiezFF1/E/Ovr6xPZQQcdlMiiusr6T57oWbt27ZrI\n2rdvn8hefvllTP/GG28ksuOPPz6RjRkzBtPfc889iaxt27Z4bffu3RPZsGHDEtm2bdsw/aZNmyRJ\ns2bN0uTJk/EaSerTpw/KhwwZksgWL16cyLZs2YLpx44dm8iiD6rm5uZE1qFDh0QWjVeaM956661E\n9qlPfQrT//nPf05kjzzySCKj9pekj33sY4msf//+eO0TTzyRyNq1a1f+fdttt+nKK69Uly5dMD31\nbSoXjUFJGjRoUCKbO3duIqNxIUl1dXWJ7OCDD8Zra2pqEhm1y9atWzH9uHHjEtn27dsTWTTee/To\nkcjouTZv3ozpaR5euHBh+fcPf/hDXX755ZJ4vND7IaorGltHHnlkRfeUpKeeeiqRjR49Gq89//zz\nE9lNN92UyNatW4fpzz333ESWzTd5du3ahelpLqd32cUXX4zp16xZk8iGDh2K1/7P//k/Jb09riTp\n7//+75Pr/vEf/xHTH3XUUYns7rvvxmulvdiB2rp16zsm/e7du4cDwxhjjDHmo0TpvYZyWbx4sVau\nXKmvfvWrkqQFCxbo6aef1n/9r//1HdfV1taqtrZWkjR16lStXLlSknT44Ydr7dq1+DUq8Q4QEa3Q\nGxsbE1mUV6WUSiWU09dzU1NTRWWSeFeC0ke7F/RcUV70RUS7GvlnzdpK4vqm56cvz6hcVVVVFV0n\ncb+IrqX7UhtG6aleaccz2gWlD4qoD9Fz0W5fVNasvgcMGKANGzbgNVLchyrdAaJ+KXEdRLvLBNVL\nNF5pyiJZtKtDz/D6669XnD/VYbSDQ7vD+Wft16+fnnvuOeyrUuXzGPUV6Z27XRm0qxP1y0rHq8T1\nQu0S9aEDDzwwkRWZ22gM0XNFcxOVPz8G+vfvr40bN0ri8UJEdUVjK2pDgnZ7onmoW7duiYx24d58\n801MTzv5VIfRUoKupbqmXXiJnzWq/5deeknS2+NK4p3F6ISE2mDgwIF4rbQXR3jV1dXv2M7fsmUL\nHrtNmDChfPyT/S35CC+Pj/B8hOcjPB/hET7C8xEe4SO8/eMI7z0voAYPHqyXXnpJdXV1qq6u1qJF\ni3TVVVe919u9Z6IXNU2y0cuDoBdd9OVDA5q+fGjilnbv8OwJvTyiiePQQw9NZNmX0p5Ei4VKoUUJ\nvVBWrVqF6ekrk17K0cTTqVOnRBYtLJ999tlERl/kkd4evTyov0UvH1oU0WJR4ufq2bNnIou+3rNn\naNeuXfk3vWij/Km/0wsxGgO0sKZ+IfFES4sqaqsIumf0RU9zA+UV5U9jgBb2kZzmgehDsNI2jHbs\nqV1pvES7MkV2Ogja/Yh2ZUhOY6vIs1Ib0liL8s9/yB9wwAHlv6kP0C5mtLNH6WlujMY7bTAU2UUk\nonmw0h3XaM6meqE2KLIzGM252X3btGlT/k39JdrtKtK3pb1YQFVVVenLX/6yvvvd76q5uVmnnXYa\nvsiNMcYYYz5q7JUfqOOOO07HHXfc+1UWY4wxxpgPBfZEbowxxhhTEC+gjDHGGGMK4gWUMcYYY0xB\n9koH6r2QmetWVVWpS5cuoRsDsjQhixRyFyCx9UtkKkxU6ptJYos3Kn+UP8nJ1JZMeiW26IgsiKi+\nyPojb+WQtVVUVso/8ilClpD0rHRPies1srai9qLnJ9NVid1DUPrI/wmVK7KCI0sRMu2PLJgyC5iq\nqqryb3r+KD1ZwVFbR9YvZAFUxKKFrL2K+BvaW6j80RiitoqupT6wpyuK5ubm0IKK6oDuGfmsIjmN\n92huobou4rOK0kdWcNRfaWwVeVZKH7kGaM0StFQqlf+mOqD00TuD+ksRf3iUPsqL5mK6Nno/VVqv\nkW8mqm+SRW4QKu0X0tt1WCqVyr8pfVTWqG9HeAfKGGOMMaYgXkAZY4wxxhTECyhjjDHGmIJ4AWWM\nMcYYU5B9rkSehe1obGzUli1bUFG3CIMHD0Y5xd955plnKr5vpYExJY6DReEaIiU5Cg9CyvWRp/fn\nn38+kUWKllTWFStWJLK8kmLWVhIr5FFYgSiWIcVQIrf69EwSx7ui+HoSh0IhJWgK+SJxiBlyHBuF\nIMgCW+aJYtmR0jwp10fhgLIwP7t27Sr38yJK5FSvNF6iWHjUt6OYY6QYS0qxUSw6Sk+KspEhA5WV\nro3qmuaGKMwS9YE9FcZLpVL4rKTcTW1A4aAk7gNZkNU8kWI1ja1IsZmMBqis0RgYMGBAIqOxFc1t\nlQZwj+qalOvz75HGxsby3zReiyisU73QGIyg91v0fiClfSpXFAuPYr3S+yka7/SOpzaIjE6oXqM5\nNwsb09TUVP5N6aOQZlGIlwjvQBljjDHGFMQLKGOMMcaYgngBZYwxxhhTEC+gjDHGGGMK4gWUMcYY\nY0xB9rkVXhYGoVQq6YADDtDHPvYxvG7r1q2JbPjw4YmMrBEkDuVSJAQEWS5EVjkUWoEsOoq4qu/b\nt28iiyyoyMqhV69eeC3VQU1NTSLLW99kbRWVgZ41CsWyY8eORJaFHskThfjJLCvyRBZ/VN8UMiNK\nT6ENyAKIrIckthCNQn6Q9cegQYMqKpP0tlVN27Zty21P/bJ3796YnqwzyaomspQZOHBgIovCJZBV\nDIVHicYLWcyRtRWVSeJwPGQVFVn6UDiiqA+QtVG+b7dr1059+/YN24XGFo0Nar+oXGT1S/Uvceih\nKBwR9WGyeqW5XeJnoPEehW4iKza6NgolQ+XPW1e2b9++PKZpbFO7RHVFdUDjPbL8JnlkhUfvWHq/\nRNaV1IeoXYuEtKL+Hq0FaBxGYzuTt2/fvvy7X79+yXVU1+8mj/AOlDHGGGNMQbyAMsYYY4wpiBdQ\nxhhjjDEF2SsdqK997Wvq0KGD2rRpo6qqKk2dOvX9KpcxxhhjzH7LXiuRX3/99agEHHH88cdL2q2M\nevzxx4eKoqNGjUpkL7zwQiKLFKtJiTlStCSi+xK1tbWJjBSzo/yzOsmzaNGiRLZ27VpMP3ny5EQ2\nd+5cvPa3v/1tIiPl/CFDhpR/Z20lsfIftdWSJUsw/9NPPz2RPfTQQ4ns5JNPxvSkRB2F7Fi2bFki\nI4XE0aNHY3rqb48++mgii0LB0Lig8DISK0yTwjmF9pCko48+WtJuhc3sN5UrSk9hHIYNG5bIli5d\niukpZEn0rKRAS8YgpEAtsbIwGXhs3LgR01MdUCiWyLggMnAgSJE+P47ffPNNbdiwITRwIaMLGoNR\nmCrq72RgQ31dYqMJKpPEdUhhQCJDhFWrVlWUfxQyhIwDqP6j9qPwHhR2R2LjgPXr1ycyUraWWOGa\nxkU0XskYhupP4vFC7xKaxyXpqaeeSmRU11FYNTLQoPfrgw8+iOnJkCB6P2dhgpqamsq/n3jiieS6\nyBAhClUV4SM8Y4wxxpiC7PUO1He/+11J0ic+8QlNmDBhrwtkjDHGGLO/U2qJnDdUwNatW1VdXa3X\nXntNN954oyZPnpxs+9fW1paPuKZOnaqnn35a0m6fFc8//3wYWZv8ktDWK/lQkniLL/LBQ9AWaQQd\nH1BeUWRu2tKmbfLoqIq25COfVVSHtM2dr7+srST2I0V+VaJo13QsQ88abfNTu0Z9iLbZqbtHedHx\nB92T6l/i9or87VC5aJs5OvLO2uWQQw4pH4dRW0f9mvomlTWKZE/9InpWGrPkgyYaL9QH6J7RsRj1\nF/KlFh0T0HNFPq+ovfJ59e/fXxs3bgzTU73Qc0XzYKVzXnSsFY0tgvpw9Fx7Q+Sfi9qF+nuRV17+\n+Xv37l0+PqQ2KDLeKvWFFtV/kWcgX1R0VBX5xyKov0XHlTRnFBlDVAfREVw299XU1JT9nZE6T/R+\npDmH/Ehl7NUCKs/s2bPVoUMHfeYzn3nX684++2xJ0v/6X/9Lf//3fx++EEj/gM7ps3POPdkfdaCi\nF3WlOlCRrk0RHSg6pycdqHxZs7aSKteB+v73v4/5n3jiiYmMdKCi8/i/hA7UCSecgOkr1YG69NJL\nMT21VxEdqEyXKU+kB5c5d/zyl7+sn/70p2H+0SRFcnLGGulAkXPJv5QOFL0QaEKkMSixXsnjjz+e\nyCIdKJIffvjheC3plj322GPl3z/84Q91+eWXv0PnMA/pmtAYjJxT0uRPL/VIB4o+boo49CUdqAgq\nVxEdKPqQKaIDRXnlx+W3vvUt3XzzzZLY6WMRHSiah2i8RDpQtICJXuXjx49PZHfffXcii+Zcqpci\nOlDLly9PZPSsUV8hHaijjjoKr/3JT34iSbrxxht17bXXShKuSf74xz9ietJbnTFjBl4r7cUR3q5d\nu9TS0qKOHTtq165deuyxx/T5z3++1XTZwmDHjh1atGgRDjopnnz3JHp5UmcmD7wR9OUWfeXRhEoT\n+sqVKzE9DaZNmzZVlI/Eyndr1qzBa0mJmzpT/vmztpJ48qaXf+TVduHChYmM+gBdJ0ndunVLZNGu\nCCm1UrtGyouUnjz4Rl/51AZRH6zUm3v0wZDtEJ5//vl6+OGHJXG7kEKpxN6GabGU5bMnNF5Xr16N\n11aqhE2TtMQvCnrWd/tyrCR91FY0D0SLvQ0bNiSy/Nd35uU/mgepvUlG/VLij7ZHHnkkkUVzG42B\n6FlpfqL00aLg2GOPTWQ0tqPxRoslkkX5U/nzyvlvvfVWuU9UmlfkiZwW7FSv0Q4W9c1oAULetWm3\nafHixZj+tNNOS2T0fnvuuecwPbUhvbcj7+I0hqIdqGzB39TUVP5N82i0GTF06FCUR7znBdRrr72m\nW2+9VdLuwp588sk65phj3uvtjDHGGGM+NLznBVSvXr30ve997/0sizHGGGPMhwK7MTDGGGOMKYgX\nUMYYY4wxBfECyhhjjDGmIHvtSLMomQVFqVRSu3btQosKsj4gi5LIWo+sDKLQDgRp+UeWKieddFIi\nIwuiyFKGfHKQRQaZzkpsGRVZW5G5OVkp5K3osraKykoWIWQCL7F15KGHHprIotAU1F/IYlFi6xMy\nK37yyScxPfUhKmvkr4gsqyKLQTINJ2uryLoxM0N/6623yr9pvETpKX8ybS8SxoRcE0hcX9Tfo7oi\neRGzboL80kTWWmRBFIU3IWulvKl0mzZt1KlTp9DfT6Um5JEFEz0XtUsUjov6S1RWGi/UVpF1JY1t\nso6M/HtRH6a+Fs3jdN98+Zubm8t/Vxp6KXJjUKnlcuRLjeo1yovmAXq/RaFgqAxUr5GFMFHEkrWI\nS6H8mM9+U/nJNUJUrnfDO1DGGGOMMQXxAsoYY4wxpiBeQBljjDHGFMQLKGOMMcaYguxzJfJMea65\nuVm7du0KFTUpjhVdGylvkrJzpFhNkEJgpDxJ4QpIgbiurg7TU1wfCsYbKRRSCIIolAuVleoqr5CY\ntZXEdUiyFStWYP6kaEpligJbUriEyJCA6psULYukJ1mkmE3tFcVDJAVWujZSfsza4IADDij/phBB\nUWgIupbaNVLYp7ERKSZTXpQ+Uham8BiklFtEMZvKFCmvkrxIjEFSTI76e6XBr6OQVvRcUbtUSqSc\nT4rNlQYflypXrC7yrEUUkMlAIl/+UqlU/rvSYPdRWSsNfBzNLVSvUV7ULvR+iuZBqlcqVzS3Vapc\nHs1tRQwZ8krg2W8qf6VBtlvDO1DGGGOMMQXxAsoYY4wxpiBeQBljjDHGFMQLKGOMMcaYguxzJfIu\nXbrszviAA9SlS5fy33tCSl6kfBh5LyXlzeeee65IURMihcSjjz46kdXX11d8X1Ky69atWyIjxXpJ\nWrduXSKLvJbTPejavDJe1lYSK/KTx+oo/2effTaR9evXL5HRM0msKPnSSy/htaScT/1q+fLlmL57\n9+6JjDyRR/2ClBdJYT4qF3mxjpQnM2/0DQ0N5d/kYT5KT/lTu0aQAmzkiZwUYGkeIKViiZVNo+ci\nyIsyPT8p2krcrpExC5U136/atGkTzmES1yt58o6U2Om5yGiEDFGKlEniuZj6MMkkfgYa70WeNTK8\nqTR9vg/mjWlIYZzKFZWV0pMX7EjZmcZGlBe1Cyl8R4Y/ldZr9M6LIjXsCRmHRHlF5Ofi7DfNN9Hc\nUmQekbwDZYwxxhhTGC+gjDHGGGMK4gWUMcYYY0xBvIAyxhhjjClIq9pZt99+u5YvX64uXbpo2rRp\nknYrTE6fPl2vvPKKevTooWuuueZdFSGNMcYYYz5KtLqAGj9+vM466yzNnDmzLJszZ45GjBihSZMm\nac6cOZozZ44uueSSijLMrLiampq0ffv20K0/WYqQVRFZM0isTd+xY8eKyiixxV/kkp6sJ0jzPyor\nuaqnZ6V8ovtGYTCie+xJ3toua6soL6pXslSSuF3Jso2uk9gCKGrXqL4rTU/WdWS9EVmSFrGCI6tL\nsgKkUDT5vKqqqsq/6Z4kkzi8B10blZ/q4JBDDsFrqV6jMBAEWRtRuaj+JbYKIgugyHqI6iV6VrJu\ny88tbdq0UadOnQqFvSEiCyaSU1mj/GkMRdfSnEkWVNEcFD1DJWWKykXjhUKbROnz7XfAAQeU/6Z2\npX4dPROlJ1nUB+naKC8aWySLxgvdl+oqKivVC+UVzcM0t0Rlza6tqqoq/26tXSu5b0SrR3jDhg1L\ndpeWLl2qcePGSZLGjRunpUuXFsrUGGOMMebDzHvSgXrttdfKX2Fdu3ZFvyTGGGOMMR9VSi1RaO0c\ndXV1uvnmm8s6UJdeeqnuuuuu8v9PnjxZs2bNwrS1tbWqra2VJE2dOrXsrGvo0KFas2ZNuO1HW/LR\n8QVB28RRZGuCyhVtPZP+F5U/2nqmJqAjuMihGB0rRfVK25l0NJl/1qytorLSdnDUrcixIG39R5G5\nqQ6ivMgBHB0BRlHMaeuZjo+iI42tW7einKD2pryiPpg5fRwwYIA2bNggiesqOtqlvIp8GFEfiOqV\noLqOnpXGFvWBaDue7kvHOlH+VIeRw0NyxpkfmzU1NXrxxRdDZ6w0Z1G5IlUImjOpXaP5gvKPykr1\nQu0SzcM0jorM49TfabxHx6JU/ryD1KytJK5XyiuqK3K8WkTFhOb86BicjqvI+XA05/bo0SOR0bss\nOt6nNqS2IpUFqZjT0Oy58m3Vq1ev5Lrs//aE2oBUTDLekyfyLl26qL6+Xt26dVN9fX34ApGkCRMm\naMKECeW/TzvtNEnSAw88oNNOOy0c+DShDR48uOIy0qIg0qshiuhAnXTSSYmMJs7IEzpNCOTJOzq3\nzRY3eaLBlG+LjHvvvTeR5Z81ayuJB86YMWMSWTRJ0XHvMccck8iWLFmC6Ul/I8qLOj696J555hlM\nT2f/w4cPT2RUp5L0i1/8IpFFkwy19xFHHJHIIo/Xjz/+uCTppz/9qb785S9LYl2dvn37YvrDDz88\nkf3ud79LZFH5x44dm8gib/KV6kBFHqtpbqBynXPOOZie7kve6KMXSu/evRNZNMlm7ZIn/6w33HCD\nrrvuunAOJc/1tAA69thjMf2gQYMSGbVrlH9dXV3F11KUA5ovIm/8NI4o8kGUnl70tNCJIhdQuy5Y\nsKD8++abb9a3vvUtSTxeqF9H+pGLFi1KZMOGDUtk0cJ29erViYzGoCSdf/75iezmm29OZA8//DCm\nv+yyyxIZLUCiuYnem1TXF110EaZ/+umnE9mQIUPw2qlTp0p6e1xJ0lVXXZVc953vfAfTjxgxIpF9\n//vfx2ul97iAGjVqlObPn69JkyZp/vz5Gj16dMVps92aLIRBNEnSYKSKjDooLZYiRU+COm4UniT7\n4s9Dk1y0AzVy5MhERm71aSUtSYcddlgiW7hwIV67cePGREYTSn4yyoeboK8Eqpd58+Zh/tTx169f\nn8jomSRuwyiEwMsvv5zIaEKlwSzxgpfuSe0vcR+MlLjpWnp5RJM/KZHTPaMJmb4IaWd17dq1mJ7q\nKtopoDFPu3XRwpiegRZVWUibPaGxSX0wWizSTkNkhUx55dvwzTff1Lp169SzZ09MTwsYaisKkRSV\nleah6Iu80gWcxDsFNN6iFy09Ay1iaQxKPF5opyIKMUR9KH9tY2Nj+W9aRFK9Rord9Az0Lov6ID1D\n9CFI7036uInepTQ2aB6KdmFpHNLcEH1wUf7RDhSFtKL00elANA4iWl1AzZgxQ6tXr9b27dv11a9+\nVeedd54mTZqk6dOna+7cuWU3BsYYY4wxfy20uoC6+uqrUZ5tjxljjDHG/LVhT+TGGGOMMQXxAsoY\nY4wxpiBeQBljjDHGFOQ9WeHtDUceeaSk3f4WjjzySLSckKQ+ffokMrJyiDT/yeSfLPuKEOVFJqlk\nKdO/f39MT6bxL7zwQiKLrF8+9alPJbLIWossMo466qhElq+/rK0kNhWm9Jnvrz2hZ6XnontKbAUY\nuZd48sknExlZtVCZJLZKIQuyyD0GuQyILP7ovmTBE1mJDB06VNJu65TsN5k6R1Y9ZG1FlpBkFSax\nBRON4egelH9kLk/tQn0oGi8kJ59VkV8amgciqyCah/KWsI2NjXrllVdCn1VkRUdWr5FpP1kHUvrI\nkpTcS1CZJGnTpk2JjKxuI3csNGeRJWb0rAS5oyEXM1H++THYpk2b8t9kCUr9OrIyr9RvWuRKo8jc\nQHM+Pespp5yC6clikObcyM0QvbepDaicElscRhb1eZ9T2W961mhsVxr+K8M7UMYYY4wxBfECyhhj\njDGmIF5AGWOMMcYUxAsoY4wxxpiC7HMl8s2bN0varRy4efPmMGbbo48+msgo5EkU8JOU0ymMSQQp\nikYK7xRzi8IaRLHwfvKTnySyiRMnJrJI0fTHP/5xIjv11FPx2tNPPz2R/fa3v01kWTtJb7eVxO1C\nsfii+HAU4oViCVLMPImVFyOFwEyZOg8FDKWwORIr255wwgmJ7BOf+ASm/z//5/8ksiiEwIABAxIZ\nhX0ZNWoUpr///vsl7VbmzuqOnj+KhZcZCeT56U9/msgotprEcbiieiVFeso/Us6nPkBj45Of/CSm\nJyVyCpMUKQBT2JXIOIDyyscm69atm84///wwJigp0ObHZsa5556L6QcOHJjIZs+enciicFxFlPv7\n9euXyKgOqfyS9JnPfCaRUf1FoVhIsZhCyUTK1jQ2/uM//qP8u127duWYhxQL7/jjj09kUV3lY+xl\nUCy8KPg3zcPRnH/BBRckMpqH5syZg+n/8R//MZFRHUYGJlQuGkOXXnoppidjoCjU16pVqyTtNp7I\n3ivnnXdect2yZcswPcVlfTe8A2WMMcYYUxAvoIwxxhhjCuIFlDHGGGNMQbyAMsYYY4wpyD5XIs+U\nv9q3b6/DDjss9LRKSnqZglgldOzYMZEdffTRFacnIoW+vKJhRqUegCXpmmuuqeiekWL117/+9YrS\nS9Jtt92WyEghLy/L2kqSTj755ORa8uQ9ffp0zJ+U4x988MFENm7cOExPitWRsu/jjz+eyMgTd5TX\n888/n8gefvjhRNbY2IjpycAhUsImxWjywvzUU09h+kyR/eCDDy7/fuKJJypOT16ASfmT2kqSFi5c\nmMhIgVliZdNnnnkmkVFbS6wwTn1g7ty5mJ6uJUMI8rovsbfiyBM59Y3f/OY35d8XX3yxfvOb3+B8\nJ0k7duxIZKSsfe+992J6UowmZWtS1JXYGCbioYceSmQjRoxIZNE8es899ySylpaWRBYpoZMxCb0H\novFKESXyXtebmprK4/TZZ59Nrl23bl0ii6JXUKQK6leRN32qw1/+8pd4LRkC/OpXv0pkkdHFv//7\nvycyMqgiQxCJFbZpbEXPSmMgilSxcuVKSbuNabLf1K/I67skLV++HOUR3oEyxhhjjCmIF1DGGGOM\nMQXxAsoYY4wxpiBeQBljjDHGFKRVJfLbb79dy5cvV5cuXTRt2jRJuz3Z3n///WUvqxdeeKGOO+64\nv2xJjTHGGGP2E1pdQI0fP15nnXWWZs6c+Q75xIkT0fV+a2QhAzp37qzRo0eHFlRkPUIWGZlr/T0h\n65EobAxBFhEHHXQQXksWd+QqP7LAIgsccpU/ZswYTD9kyJBERlZdUmwVsid5S52srSR+1i5duiSy\n0047De9bqVVOVFdkQfTCCy/gtWTpUVVVlciiMBoUhoFCudx8882YnsJrRNZaZI362GOPJbL169dj\n+mOPPVbSbquhLHwF3XPTpk2YnvK64YYbElnUf/70pz8lsiiMBZWBLB4jaymyYHrjjTcS2R//+EdM\nT2P7iCOOqOg6ia2tovAiVF95y7BSqaSOHTuixajElmX0rJnF0Z4sXrw4kVG/jMKbUP7RnE11QM/V\np08fTP/AAw8ksp07dyYymm8ktjYjKzyaQyQufz7MVGNjYzkPstokS1Ky+pU4JBOFPorCGVHoHbIQ\nltialfpQNA+SxV6bNunhFVkNSxzCjCzqqa0lrpfVq1fjtZmFb2NjY/k3haiJLHzfdyu8YcOG4UvT\nGGOMMeavlffsB+oPf/iDFnXnfHcAACAASURBVCxYoEGDBumLX/yiF1nGGGOM+auh1ELnYntQV1en\nm2++uawDtW3btvL2/D333KP6+npdccUVmLa2tla1tbWSpKlTp+qll16StHtreMuWLehsUOKtb3Ko\nFR0p0GPRtmNEkfS0dUnHDxQZXOJjHdomp+MniY8maYtWeqdjuHcjf0yStZXEdUBHKuRoTeItdTpu\njeqqUgd0Urx9vidRu1IbUhtEjv1om5jKL3EdULkiB3DV1dWS3tlW1AbRNjttn9NRS+T4lvpbdNRC\n7UWyaGzTkS/NIzRfSDy2aQxG443yiq6l8ZZvg4EDB2r9+vXhPEj9hcZGdKxGY4D6ZXR8QvlHZaXx\nQukjR5rUX6n8UV1T/pXOV1H6/MZAv379ysdRdC2Nzej1SnM2jYHoGJvSR/2d2pvGcTaH7AnNb9Sv\nIxUXmtuoXSMVG5pbIlWIbGxl40oq1gdoHJDaSfk+4f+8C3lPwGeccUaoAyJJEyZM0IQJE8p/z5o1\nS5I0efJkzZo1q5AOFJ3x7g86UKTTsbc6UPfff3/F+ZNuVOS1/L3oQGVtJbEOFA3Q+fPn433JGzx5\nx34/dKBoQi6iA0U6EaTXU0QHKpqkSH+CXpSRDtQFF1wgSbr00kt11113SZIWLFiQXLe3OlBLlizB\n9I888kgiI6/zEus7kSzSlSGP/vTyiLymV6oDFS0AyWMyeUeXeLxlH5GSdPfdd+tLX/pSqLNIOkg0\nNrKP1D2heZD6JbV/lH80Z5MOEo3XD6sO1MyZM/W1r31NEs/ve6sDtXHjxkT2fuhAUXuTbtzFF1+M\n6W+55ZZERouS8ePHY3qKckBzc6TjSzpQw4YNw2uzeeR//+//XX4emrMjHSjS2aN2zXhPC6j6+vpy\nAZYsWRIuYojsARsaGvT888+H7ttp5U7KaNHXEH1pRwuQvU1P5aKOH616aVFC94zyp/qn9BJPKFSH\n+YkrayuJOyOlp8lA4gl5w4YNiSxa6NELIVoU0CRHAz96+VHIEVqE02Jf4i+3qL9SWWmhEO0sZv2t\nsbGx/JvuGaXv379/Iquvr09k0W4bLUyjL2KSU16RWgAteGkeqampwfS0ACAl6mi3jnYForAvlFd+\nbDQ0NGjjxo3hjistCmgRHtU1Lbbo5RUt4OhZo7zoQ4bGVrSLSe1F4z2/AM1DfZjmu6hdaWGXb7+m\npqby39Eu2p5EhhTU32lXJfq4o34V5UULRpqfI0MICh1Ec0sEPSs9F70bovypraW3jSny8yCFfYk+\nRIvS6gJqxowZWr16tbZv366vfvWrOu+887Rq1Spt2LBBpVJJPXr00GWXXfa+FMYYY4wx5sNAqwuo\nq6++OpGdfvrpf5HCGGOMMcZ8GLAncmOMMcaYgngBZYwxxhhTkPfsB+q9knlw3bVrl9asWRMqFJKi\nIikLk5KgxIprkUkoQcqLkZIeyUnJL1Jcq1QxOrJMIwuoyLPw8ccfn8jIIiOvJJi1lcTKk6QoGima\nkrUPKbVGVkGkEBkpzJOiIimnP/vss5ie+ubw4cMTWWQRQt65iyisk6lwZBadteFXvvKV8u8iXqDJ\nOpKseqJ2yTyhV3JtpUqlkfULjQ1SIj/zzDMxPY1NsgR94oknMH2lVoASe0zu1atX+Xfbtm3Vq1ev\ncG4iY5Rly5YlsiiU1oABAxIZjffIPQYpbEfK/WQZRuWnvi6xFRcpt1P9S6xcT+M9stolZeW5c+eW\nf7/55pvlPnnYYYcl11IbROM9f9+MI488MpFFFoPkyfvUU0/Fa8m6jSx0qV9I0jHHHJPIyGo2MjCh\nsU3PFVnhkXI91b/0zvkx+z1y5MjkutmzZ2P6aC6P8A6UMcYYY0xBvIAyxhhjjCmIF1DGGGOMMQXx\nAsoYY4wxpiBeQBljjDHGFGSfW+FlViHt27dHC5EMCnvSu3fvimQSh0CIrC8IskqJAhiecMIJiYys\nV6IQAmS9cNpppyWygQMHYvqFCxcmssiiIR+XMCOywMnItxVZxZC11Wc/+1m8F1m8kZVEZBlHbRDF\nxiLrEQqZEcWQIqtHausoPAqFbXn55ZfxWrL+oPRDhw7F9JlVTxYaRGKrpijkCFnhTZ06NZGdffbZ\nmH7w4MGJ7Le//S1eS6EVqK4j60oaW5Q/WYVJHF+N0kchQyoN4yGx1WZ+bHbp0kUTJ04MQ1qRJWhm\nEZvnE5/4BKan+fHRRx9NZCeeeCKmp7qKLJ8ppidZgkYWwuPGjUtkZMVHVssSWwdS6KYozBSNrbx1\nZefOnctWzHlLyowhQ4Yksshym8Y2WQFGMdsopFTk4JreJWTF97Of/QzTT548OZFRG0bzKM3DNI+f\nccYZmJ76W2SJmY2Dgw8+uPybLM+pr0mxdV+Ed6CMMcYYYwriBZQxxhhjTEG8gDLGGGOMKYgXUMYY\nY4wxBSm1RLEh/kL86Ec/krRbyfjXv/41hlWQpL59+yYycjVfU1OD6UlhOwpBQDQ0NCSygw46CK8l\npURStiZFVYld+P/xj39MZOQSX5JOPvnkRBYp9FG90H179uxZ/p21lcR1QCEUamtrMX8Kd0AKjZEy\nH4UAoNAeEocMIaXSSAmdQk6Q8uP3v/99TH/eeeclMqoricOWVFVVJTIKoSBJ559/viTpv/yX/6L/\n+3//ryQOJRPVFSkW//M//3Mii/oVhROaOHEiXkvGHPRckaIoKUaTIv//+3//D9NTvZISfxSGgxS+\nSak3kuef9bbbbtOVV14ZhrSi/kLzSPSsNDddeeWViSwKu9O9e/dEFs3ZpFhMbRjN2XfddVdF+Ufj\nlRTOm5ubE1n//v0x/dNPP53IzjnnnPLvG2+8Uddee60kNlIiwxcKUyWxwjeF+sqH1MpDBhLRnEtK\n1FRXpCwuSVdffXUiozY85ZRTMD09F/Wh6P1KIZVGjBiB12ahe7JxJXG/jIwmaBxEIW4k70AZY4wx\nxhTGCyhjjDHGmIJ4AWWMMcYYUxAvoIwxxhhjCtKqJ/JXX31VM2fO1LZt21QqlTRhwgSdffbZ2rFj\nh6ZPn65XXnlFPXr00DXXXFNISdsYY4wx5sNKq1Z49fX1qq+v16BBg7Rz505NmTJF3/zmNzVv3jwd\neOCBmjRpkubMmaMdO3bokksuaTXDzPrghz/8oS6//HJ0aS+x+3ayjItCubz66quJLLJAIihcQGRp\nQxYRlFdjYyOmJwuizJogD4W7kNhKIQohQCEf7rvvvkSWt1TK2kri0BIkI6smieuFLHUiazOy1IhC\n9FAYDarXFStWYHqy9hk7dmwii0J7UL1G4UHIMogskCgUjSQ99NBDkqQHHnigHLqB7tmnTx9MT6Fc\n7rzzzkQWhROi9PPnz8drKTzFoYcemsiikB8UDofmi3PPPRfT58NzZJAFVRR2h8ZWZBlGll35uvru\nd7+r73znO2E4JbLmpXtGIXaoXqldjzjiCExP1lLRPEihUMi6NBrbX/ziFxMZWaFFVoBkNUljMwrl\nQtaN9957b/n3z372M33hC1+QxOOA5rHIkpOsJqn+ok2JpUuXJrKoD1x00UWJ7Hvf+14i+9WvfoXp\n/+Ef/iGR0Zy7du1aTE/PQOOFrEMladmyZYmMwt5I0r/8y79Ikn7wgx/oqquukqTyuyvPlClTMD1Z\n982ePRuvlSo4wuvWrVv5xd2xY0fV1NRo69atWrp0aTmezLhx47BBjTHGGGM+ihTyA1VXV6frr79e\n06ZN0xVXXFH229HS0qLJkyejH4/a2tqyf4qpU6eWA2H2799fGzduVJQ9+euha6Ovf9rtiYL5EuQ/\nhMok8RcZ5RU9K+VVKpUSWbT7QAE/ydeNxF/P9JWbzz9rq6isJIsCL1O9UBvSbqPEzxVdS7sSVK9R\nMGDKi9qa7imxv6CoD9LOGAXUjvLKdgGHDh1aHmN0z2i8kC8w8nUT9UGq68gHDj0XlSuqK5LT2IwC\nsVJ/pR2gKH/qF9F4o53sfF3X1NToxRdfxDJJHHSWyhrtgFG90u58NF4p/2gepHtQWaPxSjuuNI9H\ndUVtQOMl2u2j8ufnxoEDB5Z9GtE4oLqO+gXNDZR/lJ52/aM+QL7EaHc18jlFpzw0NqIgz9Rf6Lko\nQLPEz0pzm/S2775+/fqVg5HnfRpmRKcWNI9F/hulCnSgMnbt2qVp06bp0ksvTSbbUqkUTuwTJkzQ\nhAkTyn9n22k+wnsbH+H5CM9HeD7CI3yE5yM8wkd4H5IjPGn3i3/atGk65ZRTNGbMGEm7KyDz9Fxf\nXx++sI0xxhhjPmq0ugPV0tKiO+64QzU1Ne9waz9q1CjNnz9fkyZN0vz583EXhjj11FMl7d5JOPXU\nUwuFclmyZEkioy8siUN+HHvssRWVUeJt3miRSEcddCQSfb1T2JIHHnggkUU7HeSCP9pVoa9qWnXn\n6y9rK4m/Jqhesh2RPZk0aVIioy/qkSNHYnrajo2+aGlnjOow2qKlcAf0/PQ1J0mf/vSnE9lJJ52E\n165bty6R0VEXXSep/LXVs2fP8u8FCxZUnJ52iyiEA30NSiqHj8kTfRHTzhKF84lCudAuILX1j3/8\nY0xPR4tHHXVUIou+/mlXJDpWoh2//Jf6rl27tHbtWgwbJPE8RnPtv/3bv2F6Oj7Jdijz0C52lD+F\nSJIqDz8VhVK57bbbEhmNwSjkB+3k0xiKjn+I/M5cmzZtyjud1F8plEqUF805c+fOTWT0/BL3AdpZ\njO6xaNGiRJbNG3vygx/8IJHREWSmE70nVFeUPjoapvc+hT+T3t5dbGhoKP+eOXNmcl20g/Xggw+i\nPKLVBdSaNWu0YMEC9evXT9/85jclSRdeeKEmTZqk6dOna+7cuWU3BsYYY4wxfw20uoA64ogjwjPA\n66677n0vkDHGGGPM/o49kRtjjDHGFMQLKGOMMcaYglTsxuD9IlOOvuCCC/TAAw+gSbHEPiFWr16d\nyCLly0jRsVLIjUFkqkz+O8jMM1J2znyL5CFl3QEDBmB6UmLPfAHtCSkfkkls3ndI1lYSm9GTommU\nP7kxIKXcKH2PHj0SWVSvlSqQRn6gSNGRlBfPPPNMTE9KtVR+iRU9I0V6IlMYv/jii8u/V65cWXH+\nZ5xxRiIjRdPINUFmnZvnySefxGtpbJJfGHItIEmbN29OZNRWpCwdXUtK1ORCQGLl9siHDdVB3g1A\nc3OzXn/99dBAheYRctEyZMgQTE+GK48++mgii4x5yOggKispRpNyf2TIkEWpyENtRc8v8TxE81Xk\nHoMMlxYuXFj+/eabb5b7JLmUIZc60TuDlJXJlUTUrmR4c+KJJ+K148ePT2QPP/xwIps3bx6mz7sh\nyqA6jFxxEPRcp5xyCl5Lbk/I8Ep6W+G8qampbCxCrnv++Z//ueJyvRvegTLGGGOMKYgXUMYYY4wx\nBfECyhhjjDGmIF5AGWOMMcYUxAsoY4wxxpiC7HMrvCwUR9u2bdWzZ88wMCVZtZC1VhRMmCzjoujq\nxM6dOxNZZH1CcgocTMFhJQ44Sc8VPSuFS4jCYJAVFt03X6asrSS2lqLwKJFVEj0rWcpEIRCoDiNL\nTgrFQWWNLDapD1FdPfXUU5ie6jp6LgqUTRaLZJUkvTOkURYSge4ZpSer05aWlkQW9StqAxpDEgdU\npv4SBV6mMpAFUPSsFMqFxlBknVlp6CaJrSvz4YjatGmjzp07hyE7KEg7PWsUSobalZ4/aqvIiowg\nq1cKHUX9UuKykhVfkWeleTiyOKR6zffBt956q/w3vbeoD0cB5KkNKg0dtWe5WsuL6pvmQWo/ieuV\nxkZktUrvXWqDpqYmTE9zS5RXvm9kvyl9FDYmmjMivANljDHGGFMQL6CMMcYYYwriBZQxxhhjTEG8\ngDLGGGOMKcg+VyLPXKW3b99eQ4YMCRWASVmXlNmi8CakpBcpqRGkVEkK0BIrVpPyZ6SAS8qHpDxZ\nU1OD6UmxOlJWpnAH5BY/7z4/ayspVjTck0iplp6VlCcPOuggTF/kWUkJnMpPdSJxH8gra2f86U9/\nwvQUFiAqK8lJuT2q/2wctG/fvvyb6pUUoCU20KCwNdEY2rhxYyKj0BgSjw2SReEaqA+QomoU4oeU\naknhP1KgJgVeUpaWOJRK/r5VVVXq0qVLGJKKxgspMEdhNKhcffr0SWRRv6CxESnX0zxIdR3Ng9Rf\nKWxLNAaoD5PCefSsrYUMad++ffnvSkPcUJ1IHAqG+lUUeonGBr3zJK5DMtyhMkkcVouUuKM+SHlR\nG0TtQuGMIkOCbGxl40qSnnvuueS6KGTLhg0bUB7hHShjjDHGmIJ4AWWMMcYYUxAvoIwxxhhjCtKq\nQsurr76qmTNnatu2bSqVSpowYYLOPvtszZ49W/fff3/53PfCCy/Ucccd9xcvsDHGGGPMB02rC6iq\nqip94Qtf0KBBg7Rz505NmTJFRx99tCRp4sSJ+sxnPlMow0xRLvPqGim+EaQUSh5VJVaoq1QBWmKF\n9ciTOSnPkRfpSPGNnoGeNfI0S4qKpHgncR28+OKLiSz//HkPvB07dkyuJYVvUiqWWFmY8o+UJ6kO\nSUlSYk/aVNdRvZKiJNVf1K6kwBspJpMCLCnwrl69GtNn7dXc3Fz+TfeMPF5XV1cnMurvmzdvftf8\n80TefkmxmOoq8txPyrrk2Tjy4kz5k6JpVH5SoiZl8ahc+bwaGhr03HPPhUrkNI/1798/kRXxpk/j\nIpovqK6iOZv6AI03yl/i/kZjK+rD1C+i9wNB83jemKe5ubn8N7VB5B2boGcgwxuqU4mNOSKDLJrf\naM6NlPupDahckTEOzVk0D0bzMLVhZNCVvXcaGhrKv8ePH59cF42XItFKpAoWUN26dStbJ3Ts2FE1\nNTU4MRtjjDHG/LVQSAeqrq5O69evL5tQ/uEPf9A3vvEN3X777eHq1xhjjDHmo0apJdpP3YNdu3bp\n+uuv1+c+9zmNGTNG27ZtK2+Z3nPPPaqvr9cVV1yRpKutrVVtba0kaerUqVq/fr2k3b5INm3aVOhY\ni7bOyceExFucdHwUQeWKfHpQFdJWYLSdTM9AC9Ii6aPgoHQtbV3nj6qytpL4WIPqJcqf/B1RXUXH\nrSSPgpMS1AeiIUB9iMofHWlUWldS622QEdVrtktcU1NT3p6nPhRtUdNRD8kiH0BUV5Re4vYq0gdo\nHFD+UV3TtdE8RNC1ReahPIMHD9YzzzwT9mHqQ9QHo3wqPcKK2rXSupa4vejaImObjnWitqrUH19U\n163NTUOGDNHTTz8d3pdkUR+kMlR6T6ly9QSJj+eLBMTe2/FKcxbVSz7Idh7yiRjNLdlzDR06tOy/\nilQBooDS1N+OPfZYvFaqcAHV2Niom2++WSNHjtQ555yT/H9dXZ1uvvlmTZs2rbVbafLkyZKk6667\nTjfccEP48iFnb8uXL09kkfOvv4QOVKS/UqkOVORYkJyBLly4MJFFZ9RUB6tWrao4L9L/yHe6rK2k\nynWgHn/88YrzL6IDRefsVH7pL6MDRboPCxYswPQ0IUd96JlnnklkRXSgzj33XEnSjTfeqGuvvVYS\nO/iMdG3Isdzw4cMT2SOPPILpKYr54YcfjteSfl8RHSgaB0V0oOhamuQjHShamBbRgcq/FH/961/r\ns5/9bOj0k8ZbER0ocr5LC5CVK1dienqpRXM2jVkaQ1G7Un+n54ryr1QHiuYbies1f+3vf/97fepT\nn5LEY5sWBZGuDrU3vZ+KfARE74cLL7wwkd1xxx2JbNSoUZie9FlJZ25vdaBoA0biOSdyhJk914MP\nPqiTTz5ZkvSlL30pue4//uM/MD3p076bblurR3gtLS264447VFNT847FU75jL1myBD00G2OMMcZ8\nFGl1S2bNmjVasGCB+vXrp29+85uSdq9oFy5cqA0bNqhUKqlHjx667LLLKsow+3prbm7Wjh07wu1U\nWvXRFmO0lUdfiZGreIK+SCOLCHKrT9dGW9/kPp6eNVrh0+5FayE/8jz77LOJLF9/WVtJ/EVI2/9R\nWWkHhHaworairWMKgSBJ/fr1S2R01JJty+8J1SF9OX384x/H9PTlRHUt8Zcm9eGjjjoK069YsULS\n7rbIftM9o/SZZW0e2sGKjqqoDh544AG8lnYg6Os32pV54oknEhm1y4knnojp6UiA+hvtIkvcX6Pd\nKtqZy5erQ4cOGjZsGIaukngeov565plnYnralfiXf/mXRHbMMcdgehpv0VEL7TjSTnRk8XfWWWcl\nMjJYio4bqV1oDERzE+1q5HdGS6VSeU6g9qJdyGgHqtLd7WjHmuaW448/Hq8dPXp0Inv44YcTWXRq\ncPrppycyen4alxKPbXq/ReWnHdPoNGfEiBGSdu/cZr9HjhyZXHffffdherr23Wh1AXXEEUdo9uzZ\nidw+n4wxxhjz14o9kRtjjDHGFMQLKGOMMcaYgngBZYwxxhhTkMrt+t8n8qblLS0thfyvVOqPQopN\n0/eGyCcHuTGo1K+PxCax9Kx0z+i+kaIlKbdTG+zpAiD7m+qAFPmjdqnUL0tU16SsG11LZSiSnpT+\nqaxRXdO1RcKDkFJslFfWBm3atCn/pnARZBYf3ZfaNfJDRfIieVUqi6B+HZW10rwioxFSTKYxHN03\nX66Wlhbt3LmzUFlpbovqqtJ2ifKnvCJjmEoNb4r0oSL9gtqliI+61vJvbm4u/01zMaWP+gVdWyR9\nkXrZ27wqTR+9y6lcNOcWKX80NrN75NuKrm0tfaV4B8oYY4wxpiBeQBljjDHGFMQLKGOMMcaYgngB\nZYwxxhhTEC+gjDHGGGMKss+t8LKQER07dtTRRx+NYRUkDhdA1mYURkXiwJZR2BiCNP8jt/xk1ULW\nH1GAXAptQRZQFJpEkjp37pzIovAmFMZg2LBhiSz/rFlbSZVb0UWWLhQZnAKuRiF6KH23bt3wWgqD\nQMFFo/RkxUfhGhYtWoTpKRhvZIVHAU7Xr1+fyKLQDlnE8E6dOpV/k1UNhcaQpM2bNycyCu8Shdih\nsCdHHHEEXkvhTaheouCoFHKDAkdHIXrIWojmm8gqieoqyovKmh+vbdq0UefOncOwMdTeFAUimkcp\n0Da1azReKWRHZMFE1nlk1RQFgqUxQMFdo3BCVAdUJmo/iYNP5/tg27Zty39T+Cx6P0TvnF69eiUy\nCnETpaexEY1NCgZM83g2b+wJ9SGy8I2CPFO9UBtEoZtobEXBs7P3RqlUKv9+8sknk+uGDh2K6aP3\nZoR3oIwxxhhjCuIFlDHGGGNMQbyAMsYYY4wpiBdQxhhjjDEF2edK5FlogJaWFjU2NoZKcqToSUqC\npIAssVIkKRBHkBJ4FPKDFB23bduWyCg0h8TKqvSslI/EitWRUikpapJSaD6EQ9ZWEit3kxL8Y489\nhvnTtaRUHCmRk7Jx1K6k1EiGCFF6UoAl5cl169ZhelJM7tSpE15LfYsU1qn9pLcVNauqqsq/6Z6R\nsjApao4cOTKRRUqWpPA+YcIEvJaU86leI+V+UkqltiJFX4nHRpF+QfUaKUZTe+fr8K233tJLL70U\nKgBTXjU1NYls9erVmJ7mFlKgXbt2Laan8u/YsQOvJTnlHxnj0DiiOS8KERSFmNkT6msSGyIcfPDB\n5d+lUqncT0jhnMZQVFZKT+WiuVGSBg8eXFH+Eo9NUgyP+vD8+fMTGfXXI488EtO//PLLiYzmkWef\nfRbTk3J5NDaz925TU1P5N+VF73cpVsSP8A6UMcYYY0xBvIAyxhhjjCmIF1DGGGOMMQVpVQeqoaFB\n119/vRobG9XU1KSxY8fqvPPOU11dnWbMmKHt27dr0KBBuvLKK1GPwBhjjDHmo0arK562bdvq+uuv\nV4cOHdTY2KjrrrtOxxxzjO677z5NnDhRJ510kn70ox9p7ty5OvPMM1vNMFOsbdu2rXr27BkqC5On\nVlJAjjwzk/JepMBLvPnmm4ksr1CYhxR7SQE28kROSqH0rORxXJIOPfTQRBZ52yVFSypXXpa1lcTK\nj6QUSt5vJfagSwr3kRdqKisZHEhsYEDKg1G9ksdj6ld5hfs81K6RYjTdg8oaKU8OGjRI0u4yZ7/J\ns3H0kUNK/9l98kSGDPSslF5ixWJ6rj59+mB68o5NiuHkgVji/lpkbiFjkKgP0ZyTH5uZJ3Iaw1G5\nyLggUpgn+dlnn53IIuMCyj8y3CFlYeoDffv2xfRLly5NZDTeIk/kdXV1iYzGVdQvac7KG1K0a9eu\nXPfUrqSYHXnMpigFVFaaGyXuAwsWLMBrqQ/QGOzfvz+mJ+VuasOoXskQgto16sOkRB6NzWzOaNu2\nbfk3pT/qqKMwfVTfEa0e4eVdojc1NampqUmlUkmrVq3S2LFjJUnjx4/Hzm+MMcYY81GkojO35uZm\nfetb39LmzZv1yU9+Ur169VKnTp3KX8jV1dVhjC1jjDHGmI8apRZyfhHw+uuv69Zbb9X555+vmTNn\n6rbbbpO0e1v/pptu0rRp05I0tbW1qq2tlSRNnTq1vM3arVs31dfXh747aEuftumjgJ/0WFEgV4KO\nhSLfEeTXhrZjo61nOsas9PhJ4uODyA8UQVus+frP2kriOqRyRb5iaOuW/FBFdUVHUJEvMepb9KxR\nv6A2pGuj41LyzxW1IR0ZVxq4OZ9Xvq3IDxb1VYn9KNGxXNSvSB5ts1MZ6Lmi40qS03iNfOhUeqQQ\ntRX1q6gPkTz//P3799fGjRvDeZD6O42N6FiN+hX5J4uO8Cj/qKzUrpQ+mrPpI5zqLzqGjvp2pflT\nH8yrbfTt27d8LE5zA6WP1AtIFYLaKqrrIv656MiZ8qLjWomPwKgO6ZmivOi5igSJjvxrZe/9bFxJ\n3AaR6hCNAzpuzSik9d25c2cNHz5ca9eu1RtvvKGmpiZVVVVp69at+LKQdjvTyzvUmz17tiTpvPPO\n0+zZs8OBT2esixcvTmSR7gB15n2pA0Vn33RuLbFju2zRmSdyxJkdpeYpcqRKL6S8rlHWVlLlOlDk\nfE1i54ykqxKdp5MOy0anjwAAIABJREFUFOn6SHuvA0X6PnTtTTfdhOkvvPDCRBbpQJETQSprFLH8\nggsukPTOtiKdiE2bNmF60oG64YYbEtmSJUswPfW3iRMn4rXUXiTbWx2o3//+95ieXsCkExEtAElP\n4r3qQP3whz/U5ZdfHi726KVGY4PmC4n1Sv7bf/tvieyJJ56oOP8iOlCUPtKBuueeexIZvSgPOeQQ\nTF+pDlSk60M6UGeddVb59y233KJ/+Id/kMTPSjpQ0cL05JNPTmTUVpFOzujRoxNZpAN12mmnJbIN\nGzYksksuuQTTX3311YmM2pCeSWJHntTfo/fjsmXLElmkw5QtzO6880793d/9nSTur1F6mgcfffRR\nvFaqQAfqz3/+c3lyamho0GOPPaaamhoNHz68vKCZN2+eRo0a1dqtjDHGGGM+ErS6A1VfX6+ZM2eq\nublZLS0tOuGEE/Txj39cffv21YwZM/TLX/5SAwcO1Omnn15RhtlXXVVVVfiFJ/FuD+10RPegHaho\n248osgNFW6e0bRiVtdJnjbZIaWcqyouOL+iLPJ8+31b0pU2yKH+SF2lXqquoXWj7n2TRziQdCVAb\nRO1C5YquJTltc0c7HZk8s+qK7hmlp/qmeonahfKK6pXKQHXV2rO2RnTUU+lzRTu+lR5VSa3v2Gah\nd6KjGqqXIu1SabsW6cMRNGfSUUnUL6i+KX30rJQ/qQdE+dM8lO9r+bFFx01Urui4kOqbnjVqlyJz\nLr0Li7xf6L50bfSs1IcqVXuReLcq2rHNdq2rqqrK+dLudjS2i5xSSRUsoPr3769bbrklkffq1Ss8\nujDGGGOM+ShjT+TGGGOMMQXxAsoYY4wxpiBeQBljjDHGFGSfB69bsWKFpN3ey1esWBGaxL7yyith\n2jxbtmzB9EWUB4kiSuRk0kom8JHiG/ngoWclxT+JlfcovcQKgZR/3l1A1lYSKy+SqXJk+knK9WvW\nrElkkfluETcGpNxPrgEi4wJqQ3JDEPlmojaI3H2QCTO1N9WVJD3++OOSdrsNyX5n/+ahvirxeFm9\nenUii/oVlX/t2rV4LbliIFPnyEUJuQghn1dRu1LfoueKlHLJZQK5VpDYR9hLL71U/r1z5049+uij\n4dxAfYD6deSGgMzNqQ9RX5G4v1NdS2wuTnNDZNpPz0r3jJ6V5mdSzCZ3A9G1+TGwa9eu8t80XqgP\nR3M21Te9yyIH1fQui+p18ODBieyXv/xlIiviyoJcPkTuKehdQH4aI0t+6gNHHnkkXpu589i5c6dW\nrlwpSRozZkxyHbkrkOJwMhHegTLGGGOMKYgXUMYYY4wxBfECyhhjjDGmIF5AGWOMMcYUZJ8rkWeK\nbs3Nzdq5cycqMEuspEfXRgqN5Ok0Cg5aafpIWZjKQIqe0bPSfenaIsGMo7xIeY+8IOcVErO2kjhu\nHt0zCmxJyvlUf3SdxP0iahcqQ6XBkKO8SKEx8mpLnnWj2N1ULlLYjwJu5tNnv+melaR/N1nkMZsU\nrosEAyZP3pF3b2ov8rC/t160I6gOoz5AfTsva2pq0vbt21sNjpqnSB+m+1K9RunJ6CPqw6QIT4YA\nUb+otF6juYXGCz1/NAaIfF2VSqXy31QuMuYpEmSaxlAUjJjaMMqL2pYMKaJ2pTpszWt7HiorKcHT\nGI6ujbym5++R/S4SgSQaxxHegTLGGGOMKYgXUMYYY4wxBfECyhhjjDGmIF5AGWOMMcYUxAsoY4wx\nxpiC7HMrvL/927+VJB1yyCH627/929CKrqamJpGNHDkykUXu48mqpog2PllgRaEdNm3alMjIymHg\nwIGY/rDDDktkAwYMSGSRlcPo0aMT2YgRI/BasuqgEAaHHHLIO35n7UZloPRRGA4qF4WWiOoqX64M\nCg0iccgNskih8kscRoGsPyhchyRddNFFiSyy8qAQAjQ2qF9I0qRJkyTtto7Jfvfq1Su57sUXX8T0\n1Aaf/vSnE1nULg899FAiO+OMM/DafCiTDAqbQ20t8XNRGIt/+7d/w/Q0Z3z84x9PZJGlD4WZiixB\naR479dRTy7979Oihr371q3jP7P/3hMpP10ncr84666xERuE+ovtG4beoDckyLZobfvOb3yQyGpsn\nn3wypq+rq0tkNI9/7GMfw/TUBuPGjSv/7tKlS3lMbNiwIbmWxmZkDX388ccnMgqdRBbWEr8LlyxZ\ngtdS2JK/+7u/S2Tnnnsupl+2bFkio1AqUXgVqhdqq2effRbT0/xOZZKkyy+/XNLukFkXXnihJOnO\nO+9MrrvhhhswfRb+pVK8A2WMMcYYUxAvoIwxxhhjCuIFlDHGGGNMQVrVgWpoaND111+vxsZGNTU1\naezYsTrvvPM0c+ZMrV69uuwl9Gtf+1qon2GMMcYY81Gi1QVU27Ztdf3116tDhw5qbGzUddddp2OO\nOUaS9IUvfEFjx44tlGFeibm5uTkMDUFykkVKdnRt5Kq+UqL0VAYK4xE9K1HkWStNL7FyPYVW2FPZ\nPPub6oAUs6OyUnqqqyiEQZE2rPS+kRI5XUvKwlGZSB6FKyB5FPKCyNqgVCq943elUH+hdi1S/0VC\nsRBFwmBUKouI+htB9UrK0lEZ8v2ypaUF+2n+/yu5ZzTe6d5FQlpR/lEfoLxobEX509iisCtF3hnU\nrlG/oPLnry2VSuW/qW/v7ZxN9RIZJ9AzRHmR0j9dW6ReqA8Uma8or8g4gdqwSJgmMjApMje+G63O\nMKVSqTwImpqa1NTU9L5lbowxxhjzYaSiT7Tm5mZ985vf1Fe+8hWNGDFCQ4YMkST94he/0De+8Q3d\nddddhVbaxhhjjDEfZkotBfbkX3/9dd16662aPHmyDjroIHXt2lWNjY2688471bt3b33+859P0tTW\n1qq2tlaSNHXq1LJvng4dOmjXrl3hdixtiZNfn2jbsNKt74hKj6okPhaj54qigNM2N/kAinb+yF9N\n5H+EtkPpufJb1FlbSZVHgo/8e1FkbfLVEtUVbZ0XOcYlonalrWtqg8gPVHV1dcV5UR+itoqOe7K8\nOnbsWN6ypqj10ZFApf5yon5FYzPaZqf2Ilk0tqkP0Hitr6/H9NSHqV8W6RcRVNZ8+p49e6quri7s\nq5Se5sZovFG/It9O1P5R/lFZqQ2pDqPjTvK71trclKfSI/sofyp/vg93795dW7ZskcTjqIjaBs3Z\n1AbR65n6a9QHqAzkz4/mK4n9MFEdRn4WqV5IRn01ujY6cu/evbukt8eVJL388svJdZH/SDruezfd\n7kILKEn61a9+pXbt2ukzn/lMWbZq1Srde++9mjJlSqvpMwdYRx55pJ588snw3JMc0JGjsD59+mB6\nGgyRI0qCGjN6ITz33HOJ7LXXXktkgwYNwvRDhw5NZPfff38iizroiSeemMiWLl2K19JzkXPH/CSb\ntZXEA59kf/rTnzD/Y489NpGRA7kijv1eeOEFvJbagF6ekXNLcgxIi92pU6dienKkGTljJYeHNJiz\nCXxPzj//fEm7HZU+/vjjkqSFCxcm10V1RW1w7bXXJrLI0dzixYsT2Sc/+Um8lhzP0iTXs2dPTL+3\njjSpv1K/jMY79YtoGqWy5tNfccUVuv3228PFHjkT7d+/fyKbN28epl+3bl0iIyeKTz31VMX5R3M2\ntSHVdb9+/TD9Pffck8hobEb9gpwz0nwXOfIkJ7Of+MQnyr8nT56sWbNmSeI5n8YmfcRI0kknnZTI\nyJkt9WtJOu644xJZNOeSU2dyHEvzlSR9/etfT2S0ACHnnhLXC40h6qsSOziNFlsXX3yxJOnKK6/U\nbbfdJkmaMWNGct2tt96K6VesWJHIfvrTn+K1UgVHeH/+85/LX5cNDQ167LHHVFNTUx7wLS0tWrp0\nadgpjTHGGGM+arS6A7Vx40bNnDlTzc3Namlp0QknnKDPf/7z+h//43+Uv0T69++vyy67LLRmynPV\nVVdJkr7xjW/o1ltvLRTKhdy3R6El6Egh2volaDu1a9eueC19ZdCxTvTlRVuEDz74YCKLvrwOP/zw\nRJbtQuwJ7djRl2P+yzNrK4l3wegrMdqpoLA1GzduTGRRuIUioVxo67dIH6BdAaq/f/3Xf8X0tONY\nZAeKtumffvppTH/BBRdIkv77f//vuummmyTxF2kUyoX69oQJExLZI488gulpBysfBiMPtReFd4nC\nk1Ab0BikHchITrtN0VEPpaedJql1S85//dd/1d/8zd+EYSxozNMO1Nq1aytOT7JovqBrox0oqi/a\nQYnGNo1XOtaLdh9ovNB4px1Qies1X6Z///d/1+c+9zlJ/FxFrMXoWWlXJoLGRnTknM0NeX7+858n\nstNPPx3TL1q0KJHRTnamG70nNGfREegVV1yB6ek0JcorCwf0xz/+UWeeeaYknseyE5U9oXks6m9S\nBW4M+vfvr1tuuSWRX3/99a0lNcYYY4z5SGJP5MYYY4wxBfECyhhjjDGmIF5AGWOMMcYUpHKN2veJ\nTHkuC2FQxKcIKekVCStQxGMD5R/50CHfQJRX9Kx0X3rWyA9Upemlyv26ROEmqF4iHzKVlrXIdSR/\nNyW/PSkSLqFSfz+R8QTdN6orklN7R2bNmby5ubn8m+4Zpa/U51Pkc4uMCyKfUVEZ9iSqK7ovXRv5\nkarE2EWKn7XS/CXuQ3kfZ/nQOwSNTao/UqCWKm+X6FmpD0fzGNUrmbBH/YIMLIqEgqFnLeJHiuow\n7xqhpaWl3M5UL2RME7meIdP8SsskcX8jA5uoDDQ2onFJ6alclY5ridsgelbqA5GBx56h4qL0Rd5Z\n74Z3oIwxxhhjCuIFlDHGGGNMQbyAMsYYY4wpiBdQxhhjjDEF2edK5JMmTZK02/PxpEmTCsXCIw/E\nkXdvUhKrVHlUKhYLjzx5kwfdyGs6xX3LgiJWkj/FRYo8gVMsOIoDlldIzNpKYkU/UjIkr/GSNHz4\n8ERGnmrJK7DEHngjT+SkrEpKpVHgYmpDaoP58+dj+nPPPTeRRXH3yBM1eek/+uijMf1nP/tZSbvb\nKvvdu3fv5LqorqgNsvvkIU/yEsfCO+OMM/Ba8jpOXpgjpVjyjk3jPfIQT3PLqFGjElkUO5MUgCOD\nA1LOz7frgQceqFNPPVVHHHEEpqdnpXmQvEVL0vr16xPZF7/4xUQ2bNgwTE/jLYoeQQrjNLdEgVx/\n+9vfJjIab5HXd2oXUmweO3Yspqfy58dw165dy39v2LAhuZaiX0SK1eSln7xgR8r9FHcumnMpJubo\n0aMTGY13Sfr2t7+dyCh+65FHHonp6R1PcQuj+K0U/SDynJ/F6O3atWv5d+adPM/ll1+O6aP3ZoR3\noIwxxhhjCuIFlDHGGGNMQbyAMsYYY4wpiBdQxhhjjDEF8QLKGGOMMaYg+9wKb968eZJ2W33Mmzev\nkBUeaenvD1Z4ZP1BVkUbN27E9C+88EIiI8uurl27YnqyilmxYgVeS+ESyFotbwGVtZXEVnhUrsgq\niOrqmWeeSWSHHnoopierIKo/ae+t8Mgqh6wjyYJNKtaG69atS2Rk3UjWetLb9TVq1Cg99NBDkqQF\nCxYk15EFnCRt27YtkWX3yUMWMRKPTeprElsCUhuSBVsk37FjRyJ77bXXMD2NQ0p/4IEHYnq6L/WL\nKK+8defXvvY1/elPf0JL3ui+ZM0bjTey+lyyZEkii6yayBIyqleyOKM5d9OmTZie5jGyhKX5IoJC\nlkTzBc1teQu2yy67rPw3jRdqw6gPUV3RuIhCvlB/ffHFF/FasjC99957E1lk3Uh5Ze+DPFGosbVr\n11Z0z7/5m7/B9Js3b05kZAUovT3nbt++vfz7hBNOSK578MEHMX00DiK8A2WMMcYYUxAvoIwxxhhj\nCuIFlDHGGGNMQSrWgWpubtaUKVNUXV2tKVOmqK6uTjNmzND27ds1aNAgXXnllTrggH2uUmWMMcYY\ns8+peMXzu9/9TjU1NWXlt5///OeaOHGiTjrpJP3oRz/S3LlzdeaZZ7Z6n0y5u6WlRbt27Qpd1ZMS\nd3NzcyKj0CSS1NTUlMhIqTiClPxIAVliJWRSqIvCPZBCHT1rpOxMiviUXmJFeFIKzSvhZ20VQc8V\nLabpPlRXUQgEasMotAS1F/WXKH1DQ0MiI6VUUmyXOLRDBLUhKaqSTHpb4b2xsbH8u9J7Shz2hZTo\nozHUrl27REbhGiQ2sKD+SgrEEvct6lekFBxBfZDGpcTlj+YxGrP5cpVKJXXo0AH7msR9kOolCntD\nivykmBy1K43DqKzdunVLZBRKJpobSImZxnCkrEz3pWsjYx5Szs/XdVNTU/lvqkMqf2R4RHMuySJD\nCuoD0bWVlovGu8ThhOj9Ghmo0NimPhQpsVO/isZ2Ng5bWlrKv8lwJzIkiN4FERUd4W3ZskXLly8v\nx7ZqaWnRqlWryjGFxo8fH8axMcYYY4z5qFHRAuquu+7SJZdcUl7Nb9++XZ06dSp/HVRXV4dfi8YY\nY4wxHzVKLS0tLe92wSOPPKIVK1boK1/5ilatWqV7771XV1xxhb7zne/otttuk7R7S/umm27StGnT\nkvS1tbWqra2VJE2dOrXs66JXr156+eWXw6MmOiqhbeboWIu2GItA5Yq2nqkKaUufnkni4w86/onS\nk5yOQKO8aDs1/6xZW0m8JU7HYkXyp7qKjkupDaIjBSoryaI+SHIqf/TxQNdGR85Rfe1JdFSUbXN/\n7GMfK2+l0xFUlJ76UOfOnRNZdLRK5Sc/VlEZihzPUx+gMRjNAUXGNlGkv7Y2XgYOHKj169eH7U/l\nIlmkHlDpMXaR/KPxQnnRfWlcROWKnouIjvb2JJovyGdVvg8NHjy47IOq0vk96hfUN1ubh/NQXUV5\n0RFYfX19IouOxei9W2QM0LOSrIgfqtbUWQ477LCybz3yxRWppND89vGPfxyvlSrQgVqzZo2WLVum\nFStWqKGhQTt37tRdd92lN954Q01NTaqqqtLWrVtVXV2N6SdMmKAJEyaU/54+fbok6ZprrtH06dND\nPRE6z125cmUiGzRoEKans8xKB5jEFRk5BqTOQM7iyDmoJPXt2zeRkbO76Iyb7hs5d+zfv38iIwdu\n+UGXtZXEkwxNiJHDR3J8SnUVOSYkXY9Ip4EGNE1y0WCigUvl/8UvfoHpqa5pUSJxfdGLKtIz+Pzn\nPy9Juvbaa3XjjTdKYkeYkbM90oEaM2ZMIlu9ejWmp/42cuRIvJbKQJN0tAAjnQxqw0ifgfKi/hZ9\nW5LDxGhuaE0H6u6779aXvvSlcLxSuUjnjvSyonIVcdBKeUW6YaRXQzpQNN9JPDbouf5SOlDknDGv\nRzhnzhxNmjRJ0l9GB4rm4WgepDaMnPRmc0Oe2bNnJ7Jjjz0W0z/66KOJjOahaAzQs5Iu5te//nVM\nT05iBwwYgNdmmzX33XefzjnnHEnSuHHjkuueeOIJTE/z27t93La6gLrooot00UUXSVJ5B+qqq67S\n97//fS1evFgnnXSS5s2bh95OiezB27VrpwEDBoSDkRYL9KUfeSLf2wUUTchRB6EKpsEcTRy0ACLv\nq/TikPhFHX1l0YuSvmbyC+KsrbLflaSP8qf2okVNNHFEcoLKVWS3jvpm1IYEtUu0KKAvbXp50D0l\nqU+fPpJ2P1/2m+o6+qKl9qK+En3w0NiM+itRRAmc+gCVf/HixZieFjWR5/tK00dK2DS288/Vrl07\n1dTUhIt4WsDQx2q020YfgqQsHe30UBtGczZ9sFC7ZP1zTyIl5j2hHZWoXLRTEincE/kx1K5du/Lf\nlUaqiE4NaGxSH4o+uGgeinaH6V1Ii73oA58+5qlckdd1qheah6IxRLtl0TyW1Wu+rWixG82jUWSU\niPfsB+riiy/WfffdpyuvvFI7duzQ6aef/l5vZYwxxhjzoaKQ46bhw4dr+PDhknavYG+66aa/SKGM\nMcYYY/Zn7IncGGOMMaYgXkAZY4wxxhTECyhjjDHGmILs8+B1mVVH27Zt1bNnTzSLl9hKgMw0o/Sk\npR9ZCRBkPXLQQQfhtRs2bEhkZKUQuXogi4C1a9cmssjPBlnVRGE0yHy0NVPprK0krkN6LrIilNgC\nh6wsIksdskqKrCvJMows88jaTIrrsJJ7SmzZFfUBusdhhx2WyCI3BEcddZSk3VZ+2W96/sgK8LHH\nHktk5IYgslik9o7MoimMAsmiPlCpdSCNIYmt6GhuiczCi8xDZLmbf9ZSqaS2bduG1p00Nsl1SxSa\ngvrw+eefn8iiPkwWa5GlEvXNrC/miay9fvaznyUyqutoHiSLO7IupPlSUtnHU568K48OHTqUXR1Q\nu5J7hMi9RKVGV+QCQGKXC5kJ/56QGwAaA8OGDcP0d955ZyKjuS2LTLInVC80XhYuXIjpqV0iX2KZ\n5V3btm3LlvwUJeVzn/scpi8SfkvyDpQxxhhjTGG8gDLGGGOMKYgXUMYYY4wxBfECyhhjjDGmIPtc\niTxTYm3Tpo06duwYhhAgZVdSzI5iDZGya6ToWSlFFEUptELklp8U+ujaSImdFOqivEgJnBTu8/Wf\ntZXEdUBK3FG7ULvSc0V1XWksvui+pIAapadQIlR/UXqSFwmSTKEZImXffBtkv+meUfpKx0YUxJOe\nKwo6SxQJvEwUCURLfZDaukjQ2ygUCylB5/tQmzZtdOCBB4bpqQw03qKwNySnGH9RaAzKv8h4ayVW\n/TugOYPyip6V+nBUrwSVf8+2zv4mIyOac6M+XOl4iULB0H2jUGM0Z1FdRWWl+qZyReG7ov6yJ9E7\no9IwV9LugM/S28YZEoeIieq1SLg3yTtQxhhjjDGF8QLKGGOMMaYgXkAZY4wxxhTECyhjjDHGmIJ4\nAWWMMcYYU5B9boWXWRY1Nzdr586dqCGfvy4PuVnfsmULpifrj0jzvtL0kVXPyy+/nMjI+mP79u2Y\nniz26FmjuqKy7tixA6995ZVXEhmFkcjXf9ZWElthkZUGhRGR2FKDro3CeJB1ZWTtRSFiyMoic/m/\nJ9SGVK9FQgRFFilUr926dauoTFJq3RrdM7LOpLFR5LnovlEoFKpDGseRtVVkjbonUcgQsjaiMCCR\nVRLlH1lAUXs///zz5d/Nzc3asWNHaAVJZaB6oTlEYotDKmtkXUnjJaoXmt9oHEf9gqA+GD0r9eFK\nw/ZIPI/n67pNmzblvymcEM1j0ZxN5aKwOdH7jfpAlNdLL71UUV5RH6Q+RLJozqbwYdQGq1evxvQU\n6mzIkCF4bWZNWiqVyr8p7Ew0D0eWgBHegTLGGGOMKYgXUMYYY4wxBfECyhhjjDGmIF5AGWOMMcYU\npNRSxNe+McYYY4z54HagpkyZ8kFlbQritvrw4Lb6cOH2+vDgtvrwsK/aykd4xhhjjDEF8QLKGGOM\nMaYgVf/0T//0Tx9U5oMGDfqgsjYFcVt9eHBbfbhwe314cFt9eNgXbWUlcmOMMcaYgvgIzxhjjDGm\nIPs8Ft7KlSs1a9YsNTc364wzztCkSZP2dRFMwKuvvqqZM2dq27ZtKpVKmjBhgs4++2zt2LFD06dP\n1yuvvKIePXrommuuKRQnzfzlaG5u1pQpU1RdXa0pU6aorq5OM2bM0Pbt2zVo0CBdeeWV5ZhQ5oPl\n9ddf1x133KHnn39epVJJl19+ufr06eOxtR9y3333ae7cuSqVSjr00EN1xRVXaNu2bR5b+wm33367\nli9fri5dumjatGmSFL6nWlpaNGvWLK1YsULt27fXFVdc8b4d7+3THajm5mb95Cc/0be//W1Nnz5d\nCxcu1AsvvLAvi2DehaqqKn3hC1/Q9OnT9d3vfld/+MMf9MILL2jOnDkaMWKEfvCDH2jEiBGaM2fO\nB11U85/87ne/e0fQ3J///OeaOHGibrvtNnXu3Flz5879AEtn8syaNUvHHHOMZsyYoe9973uqqanx\n2NoP2bp1q37/+99r6tSpmjZtmpqbm7Vo0SKPrf2I8ePH69vf/vY7ZNFYWrFihTZv3qwf/OAHuuyy\ny/TjH//4fSvHPl1ArVu3Tr1791avXr10wAEH6MQTT9TSpUv3ZRHMu9CtW7fyyrxjx46qqanR1q1b\ntXTpUo0bN06SNG7cOLfZfsKWLVu0fPlynXHGGZKklpYWrVq1SmPHjpW0e5JxW+0fvPHGG3ryySd1\n+umnS9odNb5z584eW/spzc3NamhoUFNTkxoaGtS1a1ePrf2IYcOGJTu10VhatmyZTj31VJVKJR1+\n+OF6/fXXVV9f/76UY5/uP27dulXdu3cv/929e3c9/fTT+7IIpkLq6uq0fv16HXbYYXrttdfUrVs3\nSVLXrl312muvfcClM5J011136ZJLLtHOnTslSdu3b1enTp1UVVUlSaqurtbWrVs/yCKa/6Surk4H\nH3ywbr/9dm3cuFGDBg3SpZde6rG1H1JdXa1Pf/rTuvzyy9WuXTuNHDlSgwYN8tjaz4nG0tatW3XI\nIYeUr+vevbu2bt1avnZvsBK5Sdi1a5emTZumSy+9VJ06dXrH/5VKJZVKpQ+oZCbjkUceUZcuXWxW\n/SGhqalJ69ev15lnnqlbbrlF7du3T47rPLb2D3bs2KGlS5dq5syZuvPOO7Vr1y6tXLnygy6WKcC+\nGkv7dAequrpaW7ZsKf+9ZcsWVVdX78simFZobGzUtGnTdMopp2jMmDGSpC5duqi+vl7dunVTfX29\nDj744A+4lGbNmjVatmyZVqxYoYaGBu3cuVN33XWX3njjDTU1Namqqkpbt271+NpP6N69u7p3764h\nQ4ZIksaOHas5c+Z4bO2HPP744+rZs2e5LcaMGaM1a9Z4bO3nRGOpurpar776avm693PdsU93oAYP\nHqyXXnpJdXV1amxs1KJFizRq1Kh9WQTzLrS0tOiOO+5QTU2NzjnnnLJ81KhRmj9/viRp/vz5Gj16\n9AdVRPOfXHTRRbrjjjs0c+ZMXX311TrqqKN01VVXafjw4Vq8eLEkad68eR5f+wldu3ZV9+7dtWnT\nJkm7X9J9+/YW+nIRAAABRUlEQVT12NoPOeSQQ/T000/rzTffVEtLS7mtPLb2b6KxNGrUKC1YsEAt\nLS1au3atOnXq9L4c30kfgCPN5cuX6+6771Zzc7NOO+00fe5zn9uX2Zt34amnntJ1112nfv36lbc/\nL7zwQg0ZMkTTp0/Xq6++alPr/ZBVq/5/O3eIQiEQRWH4gGi1mRXX4FZcgwzYXcZEixuwuge74Zmt\nFotGYdzAKxcePMP/reCWA38Y5qNpmtR1nfZ9l/de13WpKAq1bas4jv99IiRt26a+73Xft7Isk3NO\nIQS29ULjOGqeZ0VRpDzP1TSNjuNgWy/hvde6rjrPU2maqq5rVVX1dUshBA3DoGVZlCSJnHMqy/In\nd/ATOQAAgBGPyAEAAIwIKAAAACMCCgAAwIiAAgAAMCKgAAAAjAgoAAAAIwIKAADAiIACAAAwegCA\nZdY78tpZRAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 720x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UWtf9Y9X5Ws",
        "colab_type": "text"
      },
      "source": [
        "#### Optimization time\n",
        "If you find spending too much time on these two steps skip them and come back later to speed up your implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGBPwOSQGYZL",
        "colab_type": "code",
        "outputId": "794b74e3-f13f-4267-8ba2-9ed2997fdc9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pip install nose"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nose in /usr/local/lib/python3.6/dist-packages (1.3.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "checksum": "300f70ef9c215a5baa6a2dfe507a11cd",
          "grade": true,
          "grade_id": "dist_tests",
          "locked": true,
          "points": 5,
          "solution": false
        },
        "id": "1qCLM1aTX5Wt",
        "colab_type": "code",
        "outputId": "6de1d662-d092-46dd-f107-974c54adf96c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        }
      },
      "source": [
        "from nose.tools import assert_less_equal\n",
        "feat=[0,1]\n",
        "knn=KNearestNeighbor(3)\n",
        "knn.train(Xtrain[:,feat],Ytrain)\n",
        "\n",
        "dists = knn.compute_distances_two_loops(Xtest[:,feat])\n",
        "one_loop_dists = knn.compute_distances_one_loop(Xtest[:,feat])\n",
        "no_loop_dists = knn.compute_distances_no_loops(Xtest[:,feat])\n",
        "\n",
        "difference = np.linalg.norm(dists - one_loop_dists, ord='fro')\n",
        "difference_1 = np.linalg.norm(dists - no_loop_dists, ord='fro')\n",
        "\n",
        "assert_less_equal(difference, 0.001, \"Two loops and one loop distance varies\")\n",
        "assert_less_equal(difference_1, 0.001, \"Two loops and no loop distance varies\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-b7c3abfa3cdb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0massert_less_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdifference\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Two loops and one loop distance varies\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0massert_less_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdifference_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Two loops and no loop distance varies\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.6/unittest/case.py\u001b[0m in \u001b[0;36massertLessEqual\u001b[0;34m(self, a, b, msg)\u001b[0m\n\u001b[1;32m   1213\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1214\u001b[0m             \u001b[0mstandardMsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'%s not less than or equal to %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msafe_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1215\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_formatMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstandardMsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1217\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0massertGreater\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/unittest/case.py\u001b[0m in \u001b[0;36mfail\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m    668\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m         \u001b[0;34m\"\"\"Fail immediately, with the given message.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfailureException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0massertFalse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: 1871.1734161162076 not less than or equal to 0.001 : Two loops and no loop distance varies"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "checksum": "d0818833671408129430944090dad13b",
          "grade": true,
          "grade_id": "time_tests",
          "locked": true,
          "points": 5,
          "solution": false
        },
        "id": "OcdjaqbqX5Wx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "b3472023-3b48-45f1-a18a-b1c21dd7943b"
      },
      "source": [
        "from nose.tools import assert_less_equal\n",
        "import time\n",
        "\n",
        "feat=[0,1]\n",
        "knn=KNearestNeighbor(3)\n",
        "knn.train(Xtrain[:,feat],Ytrain)\n",
        "\n",
        "start = time.time()\n",
        "knn.compute_distances_two_loops(Xtest[:,feat])\n",
        "end = time.time()\n",
        "two_loops = (end - start)\n",
        "print(\"Two loops took : \",(end - start))\n",
        "\n",
        "start = time.time()\n",
        "one_loop = knn.compute_distances_one_loop(Xtest[:,feat])\n",
        "end = time.time()\n",
        "one_loop = (end - start)\n",
        "print(\"One loops took : \",(end - start))\n",
        "\n",
        "start = time.time()\n",
        "no_loop = knn.compute_distances_no_loops(Xtest[:,feat])\n",
        "end = time.time()\n",
        "no_loop = (end - start)\n",
        "print(\"No loops took  : \",(end - start))\n",
        "\n",
        "assert_less_equal(one_loop, two_loops/10.0)\n",
        "assert_less_equal(no_loop, one_loop)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Two loops took :  2.068767547607422\n",
            "One loops took :  0.056665897369384766\n",
            "No loops took  :  0.0015239715576171875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qU0d5EJhX5W2",
        "colab_type": "text"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LR_JMiBIX5W3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Lets test it on the set of unseen examples...\n",
        "pclasses=knn.predict(Xtest[:,feat])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "Bjj8OnwFX5W8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(pclasses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpsLXpLaX5XK",
        "colab_type": "text"
      },
      "source": [
        "Lets see how good we are doing...\n",
        "=================="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Ct4u7JbVX5XL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Lets see how good we are doing, by finding the accuracy on the test set..\n",
        "print(np.sum(pclasses==Ytest))\n",
        "print(\"Accuracy = \", np.sum(pclasses==Ytest)/float(Ytest.shape[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "checksum": "e3441b6f30ada807d9770e6ff2e4facd",
          "grade": true,
          "grade_id": "acc_tests",
          "locked": true,
          "points": 2,
          "solution": false
        },
        "id": "Gev6vs7lX5XR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nose.tools import assert_greater_equal\n",
        "\n",
        "eat=[0,1]\n",
        "knn=KNearestNeighbor(3)\n",
        "knn.train(Xtrain[:,feat],Ytrain)\n",
        "pclasses=knn.predict(Xtest[:,feat])\n",
        "acc = np.sum(pclasses==Ytest)/float(Ytest.shape[0])\n",
        "\n",
        "assert_greater_equal(acc, 0.64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "SnAMj8gZX5XU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Lets plot the decision surface\n",
        "#%debug\n",
        "print(\" Plotting the Decision Surface of Training Set... \")\n",
        "t.plot_decision_regions(Xtrain[:,feat],Ytrain,clf=knn, res=0.02, cycle_marker=True, legend=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "AYinJJKPX5XY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\" Plotting the Decision Surface of Test Set... \")\n",
        "t.plot_decision_regions(Xtest[:,feat],Ytest,clf=knn, res=0.02, cycle_marker=True, legend=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzTShErpX5Xc",
        "colab_type": "text"
      },
      "source": [
        "# Feature Scaling (or Normalization)\n",
        "\n",
        "Since we are using Euclidean distance to find the nearest neighbours, which is (as we have seen in the lectures) is heavily influenced by differently scaled features (that features having different scales and ranges). So to make best of K Nearest Neigbhour classifier we will be needed to first scale each feature dimension. Now lets go and write code for the feature scaling in KNearestNeighbour..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJ9H9hlsX5Xe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Lets train a KNN Classifier on Normalized Petal Length and Width\n",
        "feat=[0,1]\n",
        "knn=KNearestNeighbor(3,scalefeatures=True) # train a 3-nearest neighbour classifier...\n",
        "\n",
        "knn.train(Xtrain[:,feat],Ytrain)\n",
        "#Lets test it on the set of unseen examples...\n",
        "pclasses=knn.predict(Xtest[:,feat])\n",
        "\n",
        "print(np.sum(pclasses==Ytest))\n",
        "print(\"Accuracy = \", np.sum(pclasses==Ytest)/float(Ytest.shape[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0hXPHnxX5Xi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\" Plotting the Decision Surface of Test Set... \")\n",
        "t.plot_decision_regions(Xtest[:,feat],Ytest,clf=knn, res=0.02, cycle_marker=True, legend=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DdhoGiEX5Xl",
        "colab_type": "text"
      },
      "source": [
        "# Comment on the effect of feature scaling and normalization. \n",
        "\n",
        "What is the difference between the results of normalized features and non-normalized features? Why we are seeing the improvement (or decrease) in the accuracy?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-isCam-X5Xm",
        "colab_type": "text"
      },
      "source": [
        "# Lets Train on all four features...."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "AtfLyafuX5Xn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Lets Train on all four features....\n",
        "\n",
        "# Lets train a Decision Tree Classifier on Petal Length and Width\n",
        "feat=[0, 1, 2, 3]\n",
        "knn=KNearestNeighbor(3)#\n",
        "knn.train(Xtrain[:,feat],Ytrain)\n",
        "pclasses=knn.predict(Xtest[:,feat])\n",
        "#Lets see how good we are doing, by finding the accuracy on the test set..\n",
        "print(np.sum(pclasses==Ytest))\n",
        "print(\"Accuracy = \", np.sum(pclasses==Ytest)/float(Ytest.shape[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "checksum": "8709cf4d5940a2b0ebeb13e2eb0c4508",
          "grade": true,
          "grade_id": "acc_all_tests",
          "locked": true,
          "points": 2,
          "solution": false
        },
        "id": "FBMqmOA_X5Xr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nose.tools import assert_greater_equal\n",
        "\n",
        "feat=[0, 1, 2, 3]\n",
        "knn=KNearestNeighbor(3)\n",
        "knn.train(Xtrain[:,feat],Ytrain)\n",
        "pclasses=knn.predict(Xtest[:,feat])\n",
        "acc = np.sum(pclasses==Ytest)/float(Ytest.shape[0])\n",
        "\n",
        "assert_greater_equal(acc, 0.90)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYgEnf-5X5Xx",
        "colab_type": "text"
      },
      "source": [
        "What can you conclude ?\n",
        "====================\n",
        "Using more features means giving more information to our classifier which results in better accuracy. Reducing a single feature means reducing a whole dimension meaning neglecting a whole dimension of maybe useful data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0y8smOoX5Xz",
        "colab_type": "text"
      },
      "source": [
        "# Cross-Validation\n",
        "\n",
        "Until now we have been splitting the dataset into a training and test set rather randomly and were reporting a rather artifical performance. Now we are going to test our system exhaustively by making use of k-fold [cross validation](http://en.wikipedia.org/wiki/Cross-validation_%28statistics%29). \n",
        "\n",
        "Now go and tune your hyper-parameters (K in this case) to opitmize the performance for only first two parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJ4-mlxhX5X0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "checksum": "112478757f94c156450112bf1d5695a6",
          "grade": false,
          "grade_id": "kfold",
          "locked": false,
          "solution": true
        },
        "id": "04zYWnYSX5X6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now lets cross validate for best paramters, and test the result...\n",
        "# We will be training four different models on four different partitions of data set and \n",
        "# then will be reporting the mean accuracy of the four classifiers.\n",
        "stime=time.time()\n",
        "\n",
        "\n",
        "nfolds=4 # lets use four folds..\n",
        "folds=t.generate_folds(X,Y,nfolds)\n",
        "features=[0,1] # features to use for our system\n",
        "#now lets train and test on these folds...\n",
        "\n",
        "#Lets perform the grid search...\n",
        "ks=np.arange(1,20,2) # search the k in the range one to 20...\n",
        "foldacc=[] # store the mean accuracy of every fold in this list\n",
        "bestMeanAccuracy = 0.0 # store the best mean accuracy across folds in this\n",
        "bestk = 0 # store the value of best k in this\n",
        "\n",
        "# YOUR CODE START HERE\n",
        "\n",
        "\n",
        "# YOUR CODE END HERE\n",
        "\n",
        "print(\"\\n\\nBest value for K = {} which gives mean accuracy = {}\".format(bestk,bestMeanAccuracy))\n",
        "etime=time.time()\n",
        "\n",
        "print('\\nTotal Time Taken ={}'.format(etime-stime))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lxTBjXcX5X_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Lets plot the accuracy w.r.t K\n",
        "plt.plot(ks,foldacc)\n",
        "plt.xlabel('K Neigbhours (Value of K\\'s)')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Cross Validation Accuracy for different Values of K')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyVf2FlqX5YI",
        "colab_type": "text"
      },
      "source": [
        "# Lets retrain the classifier with best-k and see its accuracy using only first two features..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjgRZ29BX5YL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(bestk)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1Bek0j0X5YQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feat=[0,1]\n",
        "knn=KNearestNeighbor(bestk,scalefeatures=True) # train a kbest-nearest neighbour classifier...\n",
        "knn.train(Xtrain[:,feat],Ytrain)\n",
        "\n",
        "#Lets test it on the set of unseen examples...\n",
        "pclasses=knn.predict(Xtest[:,feat])\n",
        "#Lets see how good we are doing, by finding the accuracy on the test set..\n",
        "print(np.sum(pclasses==Ytest))\n",
        "print(\"KNN Accuracy (for best K={}) ={} \".format(bestk,np.sum(pclasses==Ytest)/float(Ytest.shape[0])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUG8Gm4-X5YV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\" Plotting the Decision Surface of Test Set... \")\n",
        "t.plot_decision_regions(Xtest[:,feat],Ytest,clf=knn, res=0.02, cycle_marker=True, legend=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8Y3sicJX5Ya",
        "colab_type": "text"
      },
      "source": [
        "### Note the improvement of properly configured classifier w.r.t. to the classifier trained using only 3 Neigbours\n",
        "\n",
        "In this case the dataset is small so no improvement is shown by using bestk versus k=3 but if the dataset had been large, surely bestk would have given better results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aAgo8zWX5Yc",
        "colab_type": "text"
      },
      "source": [
        "This is the true representative accuracy measure of our system. Since, we are removing the selection bias and hence can be much more confidently use the score as a reflection of our system's performance."
      ]
    }
  ]
}